{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7eb49f6-aab2-41b5-9d07-80641fcc3fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import partialRegionBDT\n",
    "import pickle\n",
    "import uproot\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import math as mth\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from hist import Hist\n",
    "import hist\n",
    "from scipy import stats\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73d4d5-9820-4e72-ab3d-18bc1c7314b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Function to split input dataset into xgb DMatrices for test and train. Currently sets test size to 1/7 of train size. Needs to split the event weight and label features from the dataset and input them separately into the DMAtrix. This is done via two different methods - manual removal and the sklearn train_test_split function. Needs to output the weights as well, as train_test_split re-orders the lists and the corresponding weights need to be returned as well as the matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0caf9d-ea9e-4272-b8a7-786a4348c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTestTrain(inputs,features):\n",
    "   x = inputs.loc[:, features].values\n",
    "   y = inputs.loc[:, ['label']].values\n",
    "   masses = inputs.loc[:, ['massPoint']].values\n",
    "   train_input, test_input, train_lbl, test_lbl, train_masses, test_masses = train_test_split(x, y, masses, test_size=1/2.)#, random_state=0)\n",
    "   train_input_withLabel = np.concatenate((train_input, train_lbl), axis=1)  \n",
    "   signalWeights=0\n",
    "   backgroundWeights=0\n",
    "   for x in train_input_withLabel:\n",
    "        if x[len(x)-1] == 1:\n",
    "            signalWeights+=x[len(x)-2]\n",
    "        elif x[len(x)-1] == 0:\n",
    "            backgroundWeights+=x[len(x)-2]\n",
    "   shortfeat = features[0:len(features)-1]\n",
    "    \n",
    "   trainWeights = train_input[:,train_input.shape[1]-1]\n",
    "   testWeights = test_input[:,test_input.shape[1]-1]\n",
    "   train_input = np.delete(train_input,train_input.shape[1]-1,1)\n",
    "   test_input = np.delete(test_input,test_input.shape[1]-1,1)\n",
    "   dtrain = xgb.DMatrix(train_input,label=train_lbl,feature_names=shortfeat,weight=trainWeights)\n",
    "   dtest = xgb.DMatrix(test_input,label=test_lbl,feature_names=shortfeat,weight=testWeights)\n",
    "   return dtrain, dtest, trainWeights, testWeights, backgroundWeights/signalWeights, test_input, test_masses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdfe0aa-f16a-4aa0-97b0-4a4389834fa8",
   "metadata": {},
   "source": [
    "Function that splits data into an array of training features, an array of labels, and an array mass values. Allows the training features to be used independently with the BDT from the labels and mass values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "344d571d-cda5-4c5c-b90c-437531dcf46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(inputs,features):\n",
    "   trainingFeatures = inputs.loc[:, features].values\n",
    "   labels = inputs.loc[:, ['label']].values\n",
    "   signalMasses = inputs.loc[:, ['massPoint']].values   \n",
    "   return trainingFeatures, labels, signalMasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ebb345-dd22-46ab-9d9c-483f0f17aba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=6, micro=8, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ded095-d4d3-41ca-8d81-b2d6552a6a1b",
   "metadata": {},
   "source": [
    "Function that defines the negative mean squared weighted error as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42dfc715-a802-4c0e-bf69-7dbe70327e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_mean_squared_weighted_error(y_true,y_pred,weights):\n",
    "    weightedSum = 0\n",
    "    totalWeights = 0\n",
    "    for i in range(0, len(y_true)):\n",
    "        weightedSum = weightedSum + weights[i]*(y_true[i] - y_pred[i])**2\n",
    "        totalWeights = totalWeights + weights[i]\n",
    "    \n",
    "    return (-1*weightedSum)/totalWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1755fc-1aae-4cf9-a26e-f4784e2dfa9c",
   "metadata": {},
   "source": [
    "Function that trains different number of BDTs (currently 30), where the BDT parameters are randomly selected and the best \n",
    "set is determined to be the optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b99904-1924-45af-8422-000ac6f4b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import SCORERS\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def optimize_hyper_xg(inData,variables,njobs):\n",
    "    x,y,masses = format_data(inData,variables)\n",
    "    train_input, test_input, train_lbl, test_lbl, train_masses, test_masses = train_test_split(x, y, masses, test_size=0.2, random_state=0)\n",
    "    train_input_withLabel = np.concatenate((train_input, train_lbl), axis=1)  \n",
    "    signalWeights=0\n",
    "    backgroundWeights=0\n",
    "    for x in train_input_withLabel:\n",
    "        if x[len(x)-1] == 1:\n",
    "            signalWeights+=x[len(x)-2]\n",
    "        elif x[len(x)-1] == 0:\n",
    "            backgroundWeights+=x[len(x)-2]\n",
    "    print(variables)\n",
    "    shortfeat = variables[0:len(variables)-1]\n",
    "    print(shortfeat)\n",
    "    print(len(shortfeat))\n",
    "\n",
    "    trainWeights = train_input[:,train_input.shape[1]-1]\n",
    "    testWeights = test_input[:,test_input.shape[1]-1]\n",
    "    train_input = np.delete(train_input,train_input.shape[1]-1,1)\n",
    "    test_input = np.delete(test_input,test_input.shape[1]-1,1)            \n",
    "    dtrain = xgb.DMatrix(train_input,label=train_lbl,feature_names=shortfeat,weight=trainWeights)\n",
    "    dtest = xgb.DMatrix(test_input,label=test_lbl,feature_names=shortfeat,weight=testWeights)\n",
    "    \n",
    "    trainWeights_scaled = trainWeights.copy()\n",
    "    for idx, label in enumerate(train_lbl):\n",
    "        if label == 1:\n",
    "            trainWeights_scaled[idx] = trainWeights_scaled[idx]*0.0013*(backgroundWeights/signalWeights)\n",
    "    params = {'weights':trainWeights_scaled}\n",
    "    neg_mean_squared_weighted_error_scorer = make_scorer(neg_mean_squared_weighted_error, greater_is_better=True, **params)\n",
    "        \n",
    "    fixedParams = {'objective':'binary:logistic', 'booster':'gbtree'}\n",
    "    bdt = xgb.XGBRegressor(objective='binary:logistic',booster='gbtree',eval_metric='logloss',scale_pos_weight=0.0013*(backgroundWeights/signalWeights))\n",
    "    params = {\n",
    "       \"reg_alpha\": stats.uniform(0,2),\n",
    "       \"reg_lambda\": stats.uniform(0,8),\n",
    "       \"min_child_weight\": stats.uniform(0,2),\n",
    "       \"colsample_bytree\": stats.uniform(0.1,0.9),\n",
    "        \"gamma\": stats.uniform(0, 0.3),\n",
    "        \"learning_rate\": stats.uniform(0.2, 1), #stats.uniform(0, 1),\n",
    "        \"max_depth\": stats.randint(2, 6),\n",
    "        \"n_estimators\": stats.randint(60, 400),\n",
    "        \"subsample\": stats.uniform(0.5,0.5),\n",
    "    }\n",
    "    print(SCORERS.keys())\n",
    "#    search = RandomizedSearchCV(bdt, param_distributions=params, scoring = 'neg_mean_squared_error',n_iter=30, cv=5, verbose=1, n_jobs=njobs, return_train_score=False)\n",
    "    search = RandomizedSearchCV(bdt, param_distributions=params, scoring =neg_mean_squared_weighted_error_scorer,n_iter=30, cv=5, verbose=1, n_jobs=njobs, return_train_score=False)\n",
    "    print(len(train_input[:,0:len(variables)-1]))\n",
    "    print(len(variables)-1)\n",
    "    print(search)\n",
    "    out = search.fit(train_input[:,0:len(variables)-1],train_lbl,sample_weight=trainWeights)\n",
    "    print(\"Best parameter set found on development set:\")\n",
    "    print(\"\")\n",
    "    print(out.best_estimator_)\n",
    "    print(out.best_params_)\n",
    "    print(\"BEST roc_auc\")\n",
    "    print(out.best_score_)\n",
    "    return out.best_estimator_, out.best_params_, dtrain, dtest, trainWeights, testWeights, backgroundWeights/signalWeights, train_input, test_input, test_masses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69dc0c-18bf-4e74-b0fc-c4910fdcdcab",
   "metadata": {},
   "source": [
    "Function that splits the input data into nFolds that are used for training and testing. A number (nFolds) of BDTs are also created and\n",
    "trained using the training data. The function returns 2D arrays of the traing, testing, and BDTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d963aaad-ae81-45e4-b288-9a7fe15c50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import cv\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def kFoldCrossValidation(BDTparams,inData,variables,nFolds,inData_systWeights):\n",
    "    x,y,masses = format_data(inData,variables)\n",
    "    \n",
    "    random.seed()\n",
    "    seed = random.randrange(1, 10000, 1)\n",
    "    seed = 111\n",
    "    \n",
    "    kf = KFold(n_splits=nFolds,shuffle=True,random_state=seed)\n",
    "    array_train_input = []\n",
    "    array_test_input = []\n",
    "    array_test_input_systWeights = []\n",
    "    array_train_lbl = []\n",
    "    array_test_lbl = []\n",
    "    array_train_weight = []\n",
    "    array_test_weight = []\n",
    "    array_masses = []\n",
    "    array_dtrain = []\n",
    "    array_dtest = []\n",
    "    array_bdt =[]\n",
    "    idx=0\n",
    "    print(BDTparams)\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        array_train_input.append(x[train_index])\n",
    "        array_test_input.append(x[test_index])\n",
    "        array_test_input_systWeights.append(inData_systWeights.iloc[test_index])\n",
    "        array_train_lbl.append(y[train_index])\n",
    "        array_test_lbl.append(y[test_index])        \n",
    "        array_masses.append(masses[test_index])        \n",
    "        #train_input, test_input, train_lbl, test_lbl = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "        train_input_withLabel = np.concatenate((array_train_input[idx], array_train_lbl[idx]), axis=1)  \n",
    "        signalWeights=0\n",
    "        backgroundWeights=0\n",
    "        for i in train_input_withLabel:\n",
    "            if i[len(i)-1] == 1:\n",
    "                signalWeights+=i[len(i)-2]\n",
    "            elif i[len(i)-1] == 0:\n",
    "                backgroundWeights+=i[len(i)-2]\n",
    "        print(variables)\n",
    "        shortfeat = variables[0:len(variables)-1]\n",
    "        print(shortfeat)\n",
    "        print(len(shortfeat))\n",
    "        BDTparams['scale_pos_weight']=0.0013*(backgroundWeights/signalWeights)\n",
    "\n",
    "        array_train_weight.append(array_train_input[idx][:,array_train_input[idx].shape[1]-1])\n",
    "        array_test_weight.append(array_test_input[idx][:,array_test_input[idx].shape[1]-1])\n",
    "        array_train_input[idx] = np.delete(array_train_input[idx],array_train_input[idx].shape[1]-1,1)\n",
    "        array_test_input[idx] = np.delete(array_test_input[idx],array_test_input[idx].shape[1]-1,1)            \n",
    "        array_dtrain.append(xgb.DMatrix(array_train_input[idx],label=array_train_lbl[idx],feature_names=shortfeat,weight=array_train_weight[idx]))\n",
    "        array_dtest.append(xgb.DMatrix(array_test_input[idx],label=array_test_lbl[idx],feature_names=shortfeat,weight=array_test_weight[idx]))\n",
    "        #evallist = [(dtest,'eval'),(dtrain,'train')]\n",
    "        #bst = xgb.train(BDTparams, dtrain,50,evallist)\n",
    "        array_bdt.append(xgb.XGBRegressor(objective='binary:logistic',booster='gbtree',eval_metric='logloss'))\n",
    "        print(BDTparams)\n",
    "        array_bdt[idx].set_params(**BDTparams)\n",
    "        print(array_bdt[idx].get_params())\n",
    "        print(len(array_test_input[idx][:,0:len(variables)-1]))\n",
    "        print(len(array_test_lbl[idx]))\n",
    "        eval_set = [(array_test_input[idx][:,0:len(variables)-1],array_test_lbl[idx])]\n",
    "        array_bdt[idx].fit(array_train_input[idx][:,0:len(variables)-1],array_train_lbl[idx],sample_weight=array_train_weight[idx],eval_set=eval_set)\n",
    "        idx = idx+1\n",
    "        \n",
    "    return array_bdt, array_dtrain, array_dtest, array_train_input, array_test_input, array_train_lbl, array_test_lbl, array_train_weight, array_test_weight, array_masses, array_test_input_systWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17b7fe-97f1-49ed-b834-7f5e052142a0",
   "metadata": {},
   "source": [
    "Function that combines the various arrays from the kFold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1317d27-0fe6-461a-94af-54cabf5a39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combiningKFolds(aBDT,aTest,aTest_array,aTestWeight,aTestMasses,aTrain,aTrain_array,aTrainWeight,nFolds, systWeights_array):\n",
    "  array_y_predsAltTrain = []\n",
    "  array_labelsaltTrain = []\n",
    "  array_y_predsAlt = []\n",
    "  array_labelsalt = []    \n",
    "  all_y_predsAlt = np.array([])\n",
    "  all_labelsalt = np.array([])\n",
    "  all_systWeights = pd.concat(systWeights_array)\n",
    "  all_aTestWeight = np.array([])\n",
    "  all_aTestMasses = np.concatenate(np.concatenate( aTestMasses, axis=0 ), axis=0)\n",
    "  #allTest_events = np.array([])\n",
    "  allTest_events = []\n",
    "    \n",
    "  for idx in range(0,nFolds):\n",
    "    print(\"Inside combiningKFolds\")\n",
    "    y_predsAlt = aBDT[idx].predict(aTest_array[idx])\n",
    "    all_y_predsAlt = np.concatenate([all_y_predsAlt,y_predsAlt])\n",
    "    all_labelsalt = np.concatenate([all_labelsalt,aTest[idx].get_label()])\n",
    "    all_aTestWeight = np.concatenate([all_aTestWeight,aTestWeight[idx]])\n",
    "    for testEvent in aTest_array[idx]:\n",
    "        allTest_events.append(testEvent)\n",
    "#        allTest_events = np.concatenate([allTest_events,testEvent])\n",
    "    #all_aTestMasses = np.concatenate([all_aTestMasses,aTestMasses[idx]])\n",
    "    #fpralt, tpralt, thresholdsalt = roc_curve(labelsalt, y_predsAlt,sample_weight=aTestWeight[idx])     \n",
    "    \n",
    "    array_y_predsAltTrain.append(aBDT[idx].predict(aTrain_array[idx]))\n",
    "    array_labelsaltTrain.append(aTrain[idx].get_label())\n",
    "    array_y_predsAlt.append(aBDT[idx].predict(aTest_array[idx]))\n",
    "    array_labelsalt.append(aTest[idx].get_label())    \n",
    "    \n",
    "  plotOverTraining_kFold(all_y_predsAlt,all_labelsalt,all_aTestWeight,array_y_predsAltTrain,array_labelsaltTrain,aTrainWeight,\"allVariables\")\n",
    "  plotOverTraining_kFold_individaulTraining(array_y_predsAlt,array_labelsalt,aTestWeight,array_y_predsAltTrain,array_labelsaltTrain,aTrainWeight,\"allVariables\")\n",
    "\n",
    "  print(\"Length of all events used in k-fold\")\n",
    "  print(len(all_y_predsAlt))\n",
    "  print(len(all_labelsalt))\n",
    "  print(len(allTest_events))\n",
    "  print(allTest_events[0])\n",
    "  print(len(all_systWeights))\n",
    "  return all_y_predsAlt, all_labelsalt, all_aTestWeight, all_aTestMasses, allTest_events, all_systWeights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82c41c-6b9b-4767-adbb-7e6c13296d81",
   "metadata": {},
   "source": [
    "This calculates an array of HE weights for a given depth provided by the input 'sfhist'. The overall weight is found by multiplying the weights for each depth, and takes a while to calculate. Because the bin edges aren't fixed, the correct weight is found by iterating over the bins until the correct one is found rather than using a direct list access. This takes a while to calculate when iterating over every depth in every event.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cf506ec-6d10-4042-bc2d-f93b16ad6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHeDepthWeight(sfhist,heEnergy):\n",
    "   weights = []\n",
    "   for energyIndex, energy in enumerate(heEnergy):\n",
    "      lastvalue = None\n",
    "      foundbin = False\n",
    "      for idx, value in enumerate(sfhist.values()):\n",
    "         if(sfhist.axis().edges()[idx]>energy):\n",
    "            if idx==1:\n",
    "               heEnergy[energyIndex]=0\n",
    "            weights.append(lastvalue)\n",
    "            foundbin = True\n",
    "            break\n",
    "         lastvalue = value\n",
    "      if not foundbin:\n",
    "         weights.append(lastvalue)\n",
    "   return np.array(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a61c8d-40b5-4bcc-a06e-22f8cc8e7d05",
   "metadata": {},
   "source": [
    "Function that calculates CSC reweight values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e13f70-a8b7-4de4-bf6e-f38d0013415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getcscStationWeight(sfhist,cscSegmentDR):\n",
    "    weights = []\n",
    "    for cscIndex, cscDR in enumerate(cscSegmentDR):\n",
    "      if cscDR > 2:\n",
    "        cscDR = -1      \n",
    "      for idx, value in enumerate(sfhist.axis().edges()):\n",
    "        if(cscDR<value):\n",
    "            cscWeight = sfhist.values()[idx-1]\n",
    "            break\n",
    "      weights.append(cscWeight)\n",
    "    return np.array(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca84e79-e35e-4042-aa42-a06d4bcb3312",
   "metadata": {},
   "source": [
    "This function plots all of the training variables. It takes some manual manipulation to change the scale of the plots to make them look nice for every variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7b32be2-ebb9-4b66-b29a-84f99289ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotVariables(signalFiles, sigMasses, DYfile, variables):\n",
    "    xlabels = [r'$p_{T, track}$ (GeV)', r'$\\eta_{track}$', r'$\\phi_{track}$', r'$\\Delta R(track, Nearest Standalone Muon)$', \n",
    "               r'$\\Delta \\phi(track, \\mu)*charge_{track}$', r'$E_{\\mu}$', r'$\\chi^{2}_{\\mu}$', r'$\\Delta R(track, Nearest CSC segment)$',\n",
    "               r'refitStaE', r'disStaChi2', r'refitStaChi2', r'refitStaDEoverE', r'disStaDEoverE', r'refitStadPhi', r'refitStadEta', r'disStadPhi', r'disStadEta', ############ new\n",
    "               r'$E_{depth 0}$ (GeV)', r'$E_{depth 1}$ (GeV)', r'$E_{depth 2}$ (GeV)', r'$E_{depth 3}$ (GeV)', \n",
    "               r'$E_{depth 4}$ (GeV)', r'$E_{depth 5}$ (GeV)', r'$E_{depth 6}$ (GeV)', r'$charge_{track}$', r'$\\Delta E(\\mu, track)/E_{track}$',\n",
    "               r'$\\Delta R(track, CSC hit station 0)$', r'$\\Delta R(track, CSC hit station 1)$',\n",
    "               r'$\\Delta R(track, CSC hit station 2)$', r'$\\Delta R(track, CSC hit station 3)$']\n",
    "    \n",
    "    binning = [[0,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100,105,110,115,120,600], \n",
    "              np.arange(-3,3,0.1),np.arange(-3.5,3.5,0.25),np.arange(0,1,0.01),\n",
    "              np.arange(-1,1,0.1), [0,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,1000],np.arange(0,15,.25),np.arange(0,.2,.005),\n",
    "              np.linspace(-1,100,50), 50, 50, np.linspace(-1,1,50), np.linspace(-1,1,50), np.linspace(-1,1,50), np.linspace(-0.5,0.5,50), np.linspace(-1,1,50), np.linspace(-0.5,0.5,50), ########## new\n",
    "              np.arange(-1.09,5,.1),np.arange(-1.09,5,.1),np.arange(-1.09,5,.1),np.arange(-1.09,5,.1),np.arange(-1.09,5,.1),np.arange(-1.09,5,.1),np.arange(-1.09,5,.1),\n",
    "              np.arange(-1.1,1.2,1.1),np.arange(-1,-0.6,0.01),\n",
    "              [-1,-0.5,0,0.025,0.05,0.075,0.1,0.15,0.2,2],[-1,-0.5,0,0.05,0.1,0.15,0.2,0.25,\n",
    "              6],[-1,-0.5,0,0.05,0.1,0.15,0.2,0.25,\n",
    "              6],[-1,-0.5,0,0.05,0.1,0.15,0.2,0.25,\n",
    "              6]]\n",
    "              #[-1,-0.5,0,0.025,0.05,0.075,0.1,0.15,0.2,1,2],[-1,-0.5,0,0.05,0.1,0.15,0.2,0.25,\n",
    "              #1,2,3,4,5,6],[-1,-0.5,0,0.05,0.1,0.15,0.2,0.25,\n",
    "              #1,2,3,4,5,6],[-1,-0.5,0,0.05,0.1,0.15,0.2,0.25,\n",
    "              #1,2,3,4,5,6]]\n",
    "              #np.arange(-1,6,0.1),np.arange(-1,6,0.1),np.arange(-1,6,0.1),np.arange(-1,6,0.1)]\n",
    "\n",
    "#    DYweights_unity = DYfile.loc[lambda DYfile: DYfile['eta']<0,'EventWeight']/DYfile['EventWeight'].sum()\n",
    "    DYweights_unity = DYfile['trainingWeight']/DYfile['trainingWeight'].sum()\n",
    "    for idx1, variable in enumerate(variables):\n",
    "        if idx1 == 30:continue ################################################################################# formerly 21\n",
    "        \n",
    "#        axes = DYfile.loc[lambda DYfile: DYfile['eta']<0,:].hist(column=variable, weights=DYweights_unity, bins=binning[idx1], label='DY', log=True)\n",
    "        axes = DYfile.hist(column=variable, weights=DYweights_unity, bins=binning[idx1], label='DY', log=True)\n",
    "        plt.xlabel(xlabels[idx1], loc='right')\n",
    "        for idx2, signal in enumerate(signalFiles):\n",
    "            if sigMasses[idx2] == '0p4' or sigMasses[idx2] == '0p8':continue\n",
    "#            signalWeights_unity = signal.loc[lambda signal: signal['eta']<0,'EventWeight']/signal['EventWeight'].sum()\n",
    "            signalWeights_unity = signal['trainingWeight']/signal['trainingWeight'].sum()\n",
    "#            signal.loc[lambda signal: signal['eta']<0,:].hist(column=variable, ax=axes, weights=signalWeights_unity, bins=binning[idx1], histtype='step',\n",
    "            signal.hist(column=variable, ax=axes, weights=signalWeights_unity, bins=binning[idx1], histtype='step',\n",
    "                       label=r\"$m_{A'}=$\"+sigMasses[idx2].replace('p','.')+\" GeV\")\n",
    "\n",
    "        plt.ylim([0.004,1])\n",
    "        #plt.ylim([0.001,1])\n",
    "        #plt.ylim([0.0001,1])\n",
    "        #plt.ylim([0,1])        \n",
    "        #plt.ylim([0.0004,1])        \n",
    "        #plt.ylim([0,0.3])        \n",
    "        #plt.ylim([0,0.5])        \n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid(False)\n",
    "        plt.title(\" \")\n",
    "        plt.savefig(variable+\".pdf\")\n",
    "        #fig.savefig(variable+\".png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc39de-cbb9-4bdc-91bc-dde5a63052b5",
   "metadata": {},
   "source": [
    "Set the features to pull from the datafile. Can try new trainings by adding/removing features from the lists, but need to make sure that 'EventWeight' is still the last one. Some of the functions are lazy, and assume that the last is event weight when removing it for the BDT training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8e7874d-2067-4d54-b1b6-8417ede83caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   allTrainingFeatures = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"probeCharge\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\",\"EventWeight\"]\n",
    "   allTrainingFeatures = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"probeCharge\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\",\"trainingWeight\"]\n",
    "   allTrainingFeatures_noC = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\",\"EventWeight\"]\n",
    "   allTrainingFeaturesNoEvtW = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"probeCharge\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\"]\n",
    "   allfeatures_noMissingHitBool = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"probeCharge\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\",\"cellEdgeDeta\",\"cellEdgeDphi\",\"EventWeight\"]\n",
    "   allfeatures = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"found_HEDepth_0\",\"found_HEDepth_1\",\"found_HEDepth_2\",\"found_HEDepth_3\",\"found_HEDepth_4\",\"found_HEDepth_5\",\"found_HEDepth_6\",\"probeCharge\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\",\"cellEdgeDeta\",\"cellEdgeDphi\",\"EventWeight\",\"PUupWeight\",\"PUdownWeight\",\"IDupWeight\",\"IDdownWeight\",\"ISOupWeight\",\"ISOdownWeight\",\"TrigUpWeight\",\"TrigDownWeight\",\"EnBinWeight\"]\n",
    "   featuresNoHE = [\"pt\",\"eta\",\"phi\",\"staDR\",\"staPhi\",\"staE\",\"staChi\",\"cscDR\",\"probeCharge\",\"standaloneDEoverE\",\"cscDRbyStation_0\",\"cscDRbyStation_1\",\"cscDRbyStation_2\",\"cscDRbyStation_3\",\"EventWeight\"]\n",
    "   systWeightFeatures = [\"standaloneDEoverE\",\"cellEdgeDeta\",\"cellEdgeDphi\",\"HEDepth_0\",\"HEDepth_1\",\"HEDepth_2\",\"HEDepth_3\",\"HEDepth_4\",\"HEDepth_5\",\"HEDepth_6\",\"EventWeight\",\"PUupWeight\",\"PUdownWeight\",\"IDupWeight\",\"IDdownWeight\",\"ISOupWeight\",\"ISOdownWeight\",\"TrigUpWeight\",\"TrigDownWeight\",\"EnBinWeight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66529743-faa8-4954-a7a3-e51166caf263",
   "metadata": {},
   "source": [
    "Load in the input DY file, and append a list of zeroes as a label to indicate that it is background. Also calls the 'getRocRate' function from partialRegionBDT.py to calculate the false positive rate from the previous cut-based approach for use in plotting on the ROC curves. Additionally, pulls the systematic weights (ID,ISO,trig, etc. up/down) which later get used in making inputs to combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45bf2dcf-8d5d-4829-8284-a0d4076cfc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323015\n"
     ]
    }
   ],
   "source": [
    "   inDir=\"/local/cms/user/revering/datafiles/combineInputs/histograms_23_7_5\"\n",
    "   mcFile = uproot.open(\"/local/cms/user/revering/datafiles/combineInputs/histograms_23_7_5/ZmmSim_BaseEventFiltered.root\")\n",
    "   falsePos = partialRegionBDT.getRocRate(\"/local/cms/user/revering/datafiles/combineInputs/histograms_23_7_5/ZmmSim_BaseEventFiltered.root\")\n",
    "   events=mcFile[\"demo\"][\"partialDisappear\"][\"sigVariables\"]\n",
    "   mcInputs = events.arrays(allfeatures, library=\"pd\")\n",
    "   print(len(mcInputs))\n",
    "   mcSystWeights = events.arrays(systWeightFeatures, library=\"pd\")\n",
    "   mcLabel = []\n",
    "   for entry in range(mcInputs.shape[0]):\n",
    "     mcLabel.append(0)\n",
    "   mcInputs['label'] = mcLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b78234-7ea4-43f9-b621-bedb6620f3ed",
   "metadata": {},
   "source": [
    "Input for the same sign MC events. Probably not needed anymore, but kept in case test needs to be redone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01f51cb5-6a34-4e1f-a0ae-d3f2e0c88c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "   mcFile_SameSign = uproot.open(\"/local/cms/user/revering/dphoton/slc7/CMSSW_10_6_17_patch1/src/DarkPhoton/MuAnalyzer/histograms/CRReReco_SameSign.root\")\n",
    "   events_SameSign=mcFile_SameSign[\"demo\"][\"partialDisappear\"][\"sigVariables\"]\n",
    "   mcInputs_SameSign = events_SameSign.arrays(allfeatures_noMissingHitBool, library=\"pd\")\n",
    "   mcSystWeights_SameSign = events_SameSign.arrays(systWeightFeatures, library=\"pd\")\n",
    "   mcLabel_SameSign = []\n",
    "   massPoint_SameSign = []\n",
    "   for entry in range(mcInputs_SameSign.shape[0]):\n",
    "     mcLabel_SameSign.append(0)\n",
    "     massPoint_SameSign.append(\"NA\")\n",
    "   mcInputs_SameSign['label'] = mcLabel_SameSign\n",
    "   mcInputs_SameSign['massPoint']=massPoint_SameSign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b60525-431f-4692-aa8b-ef6f25483dd0",
   "metadata": {},
   "source": [
    "Input for the same sign data events. Used to estimate the muPX background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "397ad6ea-fd32-4f11-ad98-7c720ea87e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "   dataFile_SameSign = uproot.open(\"/local/cms/user/revering/datafiles/combineInputs/histograms_23_6_14/sameSignCR_2018Data.root\")\n",
    "   events_data_SameSign=dataFile_SameSign[\"demo\"][\"partialDisappear\"][\"sigVariables\"]\n",
    "   dataInputs_SameSign = events_data_SameSign.arrays(allfeatures_noMissingHitBool, library=\"pd\")\n",
    "   dataLabel_SameSign = []\n",
    "   massPoint_SameSign = []\n",
    "   for entry in range(dataInputs_SameSign.shape[0]):\n",
    "     dataLabel_SameSign.append(0)\n",
    "     massPoint_SameSign.append(\"NA\")\n",
    "   dataInputs_SameSign['label'] = dataLabel_SameSign\n",
    "   dataInputs_SameSign['massPoint']=massPoint_SameSign   \n",
    "   dataInputs_SameSign['staPhi']=dataInputs_SameSign['staPhi']*dataInputs_SameSign['probeCharge']\n",
    "   dataInputs_SameSign['trainingWeight'] = dataInputs_SameSign['EventWeight']    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d547cbd1-0975-4163-9a90-b49533dc024a",
   "metadata": {},
   "source": [
    "Input the high HCAL events, both data and MC. Probably not needed anymore but kept in case the data vs MC check needs to be redone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f10548b0-aa39-49d0-921e-273b688869d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "   mcFile_highHCAL = uproot.open(\"/local/cms/user/revering/dphoton/slc7/CMSSW_10_6_17_patch1/src/DarkPhoton/MuAnalyzer/histograms/highHcal_mc.root\")\n",
    "#   mcFile_highHCAL = uproot.open(\"/data/cmszfs1/user/krohn045/DarkPhoton/MichaelsCode/CMSSW_10_6_17_patch1/src/DarkPhoton/MuAnalyzer/HighHCAL_CR/DYJets/hists/highHcal_mc.root\")\n",
    "   events_highHCAL=mcFile_highHCAL[\"demo\"][\"partialDisappear\"][\"sigVariables\"]\n",
    "   mcInputs_highHCAL = events_highHCAL.arrays(allfeatures_noMissingHitBool, library=\"pd\")\n",
    "   mcSystWeights_highHCAL = events_highHCAL.arrays(systWeightFeatures, library=\"pd\")\n",
    "   mcLabel_highHCAL = []\n",
    "   massPoint_highHCAL = []\n",
    "   for entry in range(mcInputs_highHCAL.shape[0]):\n",
    "     mcLabel_highHCAL.append(0)\n",
    "     massPoint_highHCAL.append(\"NA\")\n",
    "   mcInputs_highHCAL['label'] = mcLabel_highHCAL\n",
    "   mcInputs_highHCAL['massPoint']=massPoint_highHCAL\n",
    "    \n",
    "   dataFile_highHCAL = uproot.open(\"/local/cms/user/revering/dphoton/slc7/CMSSW_10_6_17_patch1/src/DarkPhoton/MuAnalyzer/histograms/highHcal_data.root\")\n",
    "#   dataFile_highHCAL = uproot.open(\"/data/cmszfs1/user/krohn045/DarkPhoton/MichaelsCode/CMSSW_10_6_17_patch1/src/DarkPhoton/MuAnalyzer/HighHCAL_CR/Data/hists/highHcal_data.root\")\n",
    "   events_highHCAL=dataFile_highHCAL[\"demo\"][\"partialDisappear\"][\"sigVariables\"]\n",
    "   dataInputs_highHCAL = events_highHCAL.arrays(allfeatures_noMissingHitBool, library=\"pd\")\n",
    "   dataSystWeights_highHCAL = events_highHCAL.arrays(systWeightFeatures, library=\"pd\")\n",
    "   dataLabel_highHCAL = []\n",
    "   massPoint_highHCAL = []\n",
    "   for entry in range(dataInputs_highHCAL.shape[0]):\n",
    "     dataLabel_highHCAL.append(0)\n",
    "     massPoint_highHCAL.append(\"NA\")\n",
    "   dataInputs_highHCAL['label'] = dataLabel_highHCAL\n",
    "   dataInputs_highHCAL['massPoint']=massPoint_highHCAL    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce06645-4076-4bdb-aa3a-e474d9a34a53",
   "metadata": {},
   "source": [
    "Function to merge the signal files, while keeping the weights the same across each signal point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "552e8a98-3a99-4797-9179-8baf7240c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeSignalFiles(signalArray):\n",
    "    maxWeight = 0\n",
    "    for signal in signalArray:\n",
    "        if maxWeight < signal['EventWeight'].sum():\n",
    "            maxWeight = signal['EventWeight'].sum()\n",
    "    for signal in signalArray:\n",
    "        signal['trainingWeight'] = (maxWeight/signal['EventWeight'].sum())*signal['EventWeight']\n",
    "        print(\"CHECKING REWEIGHTING OF SAMPLES\")\n",
    "        print(signal['EventWeight'].sum())\n",
    "        print(len(signal['EventWeight']))\n",
    "    return pd.concat([signalArray[0],signalArray[1],signalArray[2],signalArray[3],signalArray[4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964376d-7381-43cf-96b2-adadb6c64e90",
   "metadata": {},
   "source": [
    "Load in the signal files. Assumes a certain filename structure (contains \"DBrem\" and has the mass just before .root). Gets the true positive rate and systematic weights in the same way as the DY loading, and labels them all with 1 to indicate signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "895947a7-9930-4140-ab79-3fc446198110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0p2\n",
      "1p0\n",
      "0p4\n",
      "0p6\n",
      "0p8\n",
      "0p8\n",
      "0p6\n",
      "0p2\n",
      "0p4\n",
      "1p0\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "18267.07831790964\n",
      "18845\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "2569.878775820811\n",
      "2651\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "3063.4657461282304\n",
      "3155\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "4307.882830158529\n",
      "4440\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "8570.154183906869\n",
      "8837\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "9176.925738220052\n",
      "9462\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "4083.039590230562\n",
      "4204\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "2919.3239197365365\n",
      "3008\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "19872.288698443277\n",
      "20487\n",
      "CHECKING REWEIGHTING OF SAMPLES\n",
      "2125.410242738193\n",
      "2186\n"
     ]
    }
   ],
   "source": [
    "   sigFiles = []\n",
    "   sigFiles_ecalBrem = []\n",
    "   sigSystWeights = []\n",
    "   sigMasses = []\n",
    "   sigMasses_ecalBrem = []\n",
    "   sigSystWeights_ecalBrem = []    \n",
    "   truePos = []\n",
    "   scaleFactor = {'0p2':800,'0p4':1200,'0p6':1600,'0p8':2500,'1p0':3200}\n",
    "   #{\"0p2\":4000,\"1p0\":40000,\"0p4\":8000,\"0p8\":16000,\"0p6\":12000,\"2p0\":80000}\n",
    "   for filename in os.listdir(inDir):\n",
    "      f = os.path.join(inDir, filename)\n",
    "      if os.path.isfile(f):\n",
    "          if \"DBrem_map\" in filename and \"fixBiasing\" in filename:\n",
    "             apmass = filename.split(\"_\")[1]\n",
    "             for mass, sf in scaleFactor.items():\n",
    "               if mass in apmass:\n",
    "                 print(mass)\n",
    "                 sigMasses.append(mass)\n",
    "                 sigEvents=uproot.open(str(inDir)+\"/\"+filename+\":demo/partialDisappear/sigVariables\")\n",
    "                 sigFiles.append(sigEvents.arrays(allfeatures, library=\"pd\"))\n",
    "                 sigSystWeights.append(sigEvents.arrays(systWeightFeatures,library=\"pd\"))\n",
    "                 truePos.append(partialRegionBDT.getRocRate(str(inDir)+\"/\"+filename))\n",
    "                 massLabel=[]\n",
    "                 massPoint = []\n",
    "                 for entry in range(sigFiles[-1].shape[0]):\n",
    "                   massLabel.append(1)\n",
    "                   massPoint.append(mass)\n",
    "                 sigFiles[-1]['label']=massLabel\n",
    "                 sigFiles[-1]['massPoint'] = massPoint\n",
    "          elif \"DBrem_map\" in filename and \"ecalBrem\" in filename:\n",
    "             apmass = filename.split(\"_\")[1]\n",
    "             for mass, sf in scaleFactor.items():\n",
    "               if mass in apmass:\n",
    "                 print(mass)\n",
    "                 sigMasses_ecalBrem.append(mass)\n",
    "                 sigEvents=uproot.open(str(inDir)+\"/\"+filename+\":demo/partialDisappear/sigVariables\")\n",
    "                 sigFiles_ecalBrem.append(sigEvents.arrays(allfeatures, library=\"pd\"))\n",
    "                 sigSystWeights_ecalBrem.append(sigEvents.arrays(systWeightFeatures,library=\"pd\"))\n",
    "                 truePos.append(partialRegionBDT.getRocRate(str(inDir)+\"/\"+filename))\n",
    "                 massLabel=[]\n",
    "                 massPoint = []\n",
    "                 for entry in range(sigFiles_ecalBrem[-1].shape[0]):\n",
    "                   massLabel.append(1)\n",
    "                   massPoint.append(mass)\n",
    "                 sigFiles_ecalBrem[-1]['label']=massLabel\n",
    "                 sigFiles_ecalBrem[-1]['massPoint'] = massPoint                    \n",
    "   allSignals = mergeSignalFiles([sigFiles[0],sigFiles[1],sigFiles[2],sigFiles[3],sigFiles[4]])\n",
    "   allSignals_ecalBrem = mergeSignalFiles([sigFiles_ecalBrem[0],sigFiles_ecalBrem[1],sigFiles_ecalBrem[2],sigFiles_ecalBrem[3],sigFiles_ecalBrem[4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b154d4-ca69-483d-9a05-df496cb8247a",
   "metadata": {},
   "source": [
    "My previous BDT training was done with the initial ~100,000 privately generated endcap muon samples, and now we have an additional ~130,000 endcap muons available. This part loads in the new files as a separate dataframe so that I can test how the BDT performs on completely independent data. In the future we should probably combine these with the original files. TO DO: Combine inputs using these files need a slightly different cross section scaling because of the gen-level filter requiring a muon in the endcap, so I need to run gen xsec analyzer (or manually find the gen matching efficiency) and include it in the total cross section scaling to get the correct expected number of events.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16789b2a-63e4-44ee-995f-fbf6a4db7fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323015\n"
     ]
    }
   ],
   "source": [
    "newGenMCFile = uproot.open(\"/local/cms/user/revering/datafiles/combineInputs/histograms_23_6_14/ZmmSim_BaseEventFiltered.root\")\n",
    "newGenfalsePos = partialRegionBDT.getRocRate(\"/local/cms/user/revering/datafiles/combineInputs/histograms_23_6_14/ZmmSim_BaseEventFiltered.root\")\n",
    "newGenEvents=newGenMCFile[\"demo\"][\"partialDisappear\"][\"sigVariables\"]\n",
    "newGenMCInputs = newGenEvents.arrays(allfeatures_noMissingHitBool, library=\"pd\")\n",
    "newGenMCSystWeights = newGenEvents.arrays(systWeightFeatures, library=\"pd\")\n",
    "print(len(newGenMCSystWeights))\n",
    "newGenMCLabel = []\n",
    "massPoint = []\n",
    "for entry in range(newGenMCInputs.shape[0]):\n",
    "  newGenMCLabel.append(0)\n",
    "  massPoint.append(\"NA\")\n",
    "newGenMCInputs['label']=newGenMCLabel\n",
    "newGenMCInputs['massPoint']=massPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5e70e5-0d8d-42e6-8f20-9b3adb846abe",
   "metadata": {},
   "source": [
    "This part calculates and applies the hcal depth-by-depth scale factors. The on-edge and off-edge splitting hasn't been added to the analyzer used to make BDT inputs yet, so these scale factors can be fairly large. The scale factor histograms are the ratio of data events in a bin of HE energy over MC events in that bin, so the total event weight is found by multiplying together these ratios across every depth. getHeDepthWeight calculates the array of weights for all events, so the overall event weight is found as the product of all six arrays. To make sure that the weight isn't applied multiple times as the sheet gets used, create a copy of the base one before applying the weight to it each time.\n",
    "Lastly, I forgot to multiply the standalone delta phi by the probe charge when I made the input trees, so I do it here. Included in this block to make sure that it can't get run twice without reloading and accidentally undo the flip.\n",
    "\n",
    "**To Check** am I putting a zero or -1 in when the event does not have an expected hit at a particular depth? Did I make sure the weight is one for those depths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79f58a1d-36a3-4328-8afb-1610b742c5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/auerb029/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "hcalSfHists = []\n",
    "for depth in range(7):\n",
    "   hcalSfHists.append(uproot.open(\"/local/cms/user/revering/dphoton/slc7/CMSSW_10_6_17_patch1/src/DarkPhoton/MuAnalyzer/jupyterstuff/hcalSFs.root:hcalSF_depth\"+str(depth)))\n",
    "inputs = []\n",
    "weightedMcInputs = mcInputs.copy()\n",
    "reweight=getHeDepthWeight(hcalSfHists[0],weightedMcInputs['HEDepth_'+str(0)])\n",
    "for depth in range(1,7):\n",
    "   reweight = reweight*getHeDepthWeight(hcalSfHists[depth],weightedMcInputs['HEDepth_'+str(depth)])\n",
    "weightedMcInputs['EventWeight']=weightedMcInputs['EventWeight']*reweight  \n",
    "weightedMcInputs['trainingWeight'] = weightedMcInputs['EventWeight']\n",
    "weightedMcInputs['PUupWeight']=weightedMcInputs['PUupWeight']*reweight  \n",
    "weightedMcInputs['PUdownWeight']=weightedMcInputs['PUdownWeight']*reweight  \n",
    "weightedMcInputs['IDupWeight']=weightedMcInputs['IDupWeight']*reweight  \n",
    "weightedMcInputs['IDdownWeight']=weightedMcInputs['IDdownWeight']*reweight  \n",
    "weightedMcInputs['ISOupWeight']=weightedMcInputs['ISOupWeight']*reweight  \n",
    "weightedMcInputs['ISOdownWeight']=weightedMcInputs['ISOdownWeight']*reweight  \n",
    "weightedMcInputs['TrigUpWeight']=weightedMcInputs['TrigUpWeight']*reweight  \n",
    "weightedMcInputs['TrigDownWeight']=weightedMcInputs['TrigDownWeight']*reweight  \n",
    "weightedMcInputs['EnBinWeight']=weightedMcInputs['EnBinWeight']*reweight  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a232a8-ac40-41bf-a5de-6c7d036f06f4",
   "metadata": {},
   "source": [
    "Calculates the CSC reweighting values and creates a new event weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f34cc0b-c4cd-4035-bd25-86f2e44b2ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "[-1.   -0.97 -0.94 -0.91 -0.88 -0.85 -0.82 -0.79 -0.76 -0.73 -0.7  -0.67\n",
      " -0.64 -0.61 -0.58 -0.55 -0.52 -0.49 -0.46 -0.43 -0.4  -0.37 -0.34 -0.31\n",
      " -0.28 -0.25 -0.22 -0.19 -0.16 -0.13 -0.1  -0.07 -0.04 -0.01  0.02  0.17\n",
      "  1.97]\n",
      "[6.053152   0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.97274566 0.86921054 1.3505379 ]\n"
     ]
    }
   ],
   "source": [
    "cscSfHists = []\n",
    "for depth in range(4):\n",
    "    print(depth)\n",
    "    cscSfHists.append(uproot.open(\"/local/cms/user/revering/dphoton/slc7/CMSSW_10_6_17_patch1/src/DarkPhoton/MuAnalyzer/jupyterstuff/cscStationSFs.root:cscScaleFactor_station\"+str(depth)))\n",
    "print(cscSfHists[0].axis().edges())\n",
    "print(cscSfHists[0].values())\n",
    "cscReweight0=getcscStationWeight(cscSfHists[0],weightedMcInputs['cscDRbyStation_'+str(0)])\n",
    "cscReweight1=getcscStationWeight(cscSfHists[1],weightedMcInputs['cscDRbyStation_'+str(1)])\n",
    "cscReweight2=getcscStationWeight(cscSfHists[2],weightedMcInputs['cscDRbyStation_'+str(2)])\n",
    "cscReweight3=getcscStationWeight(cscSfHists[3],weightedMcInputs['cscDRbyStation_'+str(3)])\n",
    "weightedMcInputs['EventWeight_cscReweight']=weightedMcInputs['EventWeight']*cscReweight0*cscReweight1*cscReweight2*cscReweight3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b563c736-e408-46d6-aeec-deb99c12f9a6",
   "metadata": {},
   "source": [
    "For all of the various samples, set HE depth energy values when hits don't exist to -1. So that the BDT distinguishes from hits with 0 energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "458252fa-f891-4e2e-a3e9-0825d1c3260a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             pt       eta       phi     staDR    staPhi        staE  \\\n",
      "0     40.688169  1.829130  2.881959  0.013875 -0.000432  122.879028   \n",
      "1     43.054313  1.746340 -0.262863  0.011356  0.011161  147.616638   \n",
      "2     40.569966  2.202706  1.433373  0.004546  0.002755  218.705139   \n",
      "3     31.126480 -2.079570 -2.041045  0.163064 -0.052736   13.001960   \n",
      "4     66.612570  1.838144  0.626287  0.051073 -0.051048   73.318413   \n",
      "...         ...       ...       ...       ...       ...         ...   \n",
      "8832  81.194240  2.128157 -2.677567  0.027747  0.025751   54.011978   \n",
      "8833  40.267019  1.579129 -2.775568  0.688508  0.588957   14.518100   \n",
      "8834  43.994075 -1.905420  0.750612  0.032933 -0.027979  226.703629   \n",
      "8835  41.336116  1.639106  1.828171  0.069064 -0.035406  258.803253   \n",
      "8836  45.013371  2.131383  0.606705  0.071117 -0.070803   60.930439   \n",
      "\n",
      "         staChi     cscDR  HEDepth_0  HEDepth_1  ...  IDdownWeight  \\\n",
      "0      0.301895  0.000858   0.247127   1.745826  ...      0.964535   \n",
      "1      0.960420  0.004398   0.182217   0.239752  ...      0.951866   \n",
      "2      0.627256  0.003335   0.863094   3.091876  ...      0.930533   \n",
      "3      1.432543  0.121063   3.172166   0.107460  ...      1.040227   \n",
      "4      0.628479  0.001246   0.309120   0.948651  ...      0.937304   \n",
      "...         ...       ...        ...        ...  ...           ...   \n",
      "8832   3.165587  0.014407   0.282289   0.511952  ...      1.013026   \n",
      "8833   2.853894  0.006740   0.171593   0.419994  ...      0.855433   \n",
      "8834  87.826294  0.004272   0.160230   0.802516  ...      0.936899   \n",
      "8835   1.413258  0.011116   0.184909   0.554984  ...      0.973260   \n",
      "8836   0.641197  0.004883   0.215138   0.848305  ...      0.981193   \n",
      "\n",
      "      ISOupWeight  ISOdownWeight  TrigUpWeight  TrigDownWeight  EnBinWeight  \\\n",
      "0        0.994186       0.955241      0.984321        0.964911     1.007736   \n",
      "1        0.981066       0.942420      0.971526        0.951766     0.994325   \n",
      "2        0.961275       0.918987      0.949690        0.930337     0.971958   \n",
      "3        1.072113       1.029736      1.061560        1.040075     1.086527   \n",
      "4        0.966058       0.928274      0.956648        0.937495     0.958557   \n",
      "...           ...            ...           ...             ...          ...   \n",
      "8832     1.044199       1.003171      1.033933        1.013232     1.016325   \n",
      "8833     0.881647       0.846813      0.872976        0.855309     0.858015   \n",
      "8834     0.967895       0.925231      0.956187        0.936702     0.928908   \n",
      "8835     1.003179       0.963881      0.993225        0.973639     0.965210   \n",
      "8836     1.011310       0.971782      1.001321        0.981575     1.000093   \n",
      "\n",
      "      label  massPoint  trainingWeight  EventWeight_cscReweight  \n",
      "0         1        0p2        0.974616                 0.974616  \n",
      "1         1        0p2        0.961646                 0.961646  \n",
      "2         1        0p2        0.940014                 0.940014  \n",
      "3         1        0p2        1.050818                 1.050818  \n",
      "4         1        0p2        0.947072                 0.947072  \n",
      "...     ...        ...             ...                      ...  \n",
      "8832      1        0p4        2.181741                 1.023582  \n",
      "8833      1        0p4        1.841899                 0.864143  \n",
      "8834      1        0p4        2.017324                 0.946445  \n",
      "8835      1        0p4        2.096162                 0.983432  \n",
      "8836      1        0p4        2.113247                 0.991448  \n",
      "\n",
      "[37928 rows x 44 columns]\n",
      "[             pt       eta       phi     staDR    staPhi        staE  \\\n",
      "0     36.234983  1.680514  0.178885  0.054614  0.048367   46.247898   \n",
      "1     48.588770 -1.953590  2.534030  0.054398  0.052517   59.415211   \n",
      "2     50.687919  2.220909  0.804715  0.068122 -0.068010  115.821770   \n",
      "3     49.187752  1.931597 -1.972739  0.021346 -0.021044  259.733459   \n",
      "4     37.035284 -2.366398  1.975624  0.096683  0.095750   44.757290   \n",
      "...         ...       ...       ...       ...       ...         ...   \n",
      "8832  81.194240  2.128157 -2.677567  0.027747  0.025751   54.011978   \n",
      "8833  40.267019  1.579129 -2.775568  0.688508  0.588957   14.518100   \n",
      "8834  43.994075 -1.905420  0.750612  0.032933 -0.027979  226.703629   \n",
      "8835  41.336116  1.639106  1.828171  0.069064 -0.035406  258.803253   \n",
      "8836  45.013371  2.131383  0.606705  0.071117 -0.070803   60.930439   \n",
      "\n",
      "         staChi     cscDR  HEDepth_0  HEDepth_1  ...  IDdownWeight  \\\n",
      "0      7.220243  0.003537   0.297983   0.743317  ...      1.563022   \n",
      "1      0.753268  0.001801   0.226028   0.493166  ...      1.858269   \n",
      "2      1.541431  0.003291   0.258848   1.403674  ...      1.132262   \n",
      "3      1.857932  0.001489   0.190599   0.000000  ...      1.126282   \n",
      "4      3.293752  0.005710   0.869334   1.777585  ...      1.031290   \n",
      "...         ...       ...        ...        ...  ...           ...   \n",
      "8832   3.165587  0.014407   0.282289   0.511952  ...      1.013026   \n",
      "8833   2.853894  0.006740   0.171593   0.419994  ...      0.855433   \n",
      "8834  87.826294  0.004272   0.160230   0.802516  ...      0.936899   \n",
      "8835   1.413258  0.011116   0.184909   0.554984  ...      0.973260   \n",
      "8836   0.641197  0.004883   0.215138   0.848305  ...      0.981193   \n",
      "\n",
      "      ISOupWeight  ISOdownWeight  TrigUpWeight  TrigDownWeight  EnBinWeight  \\\n",
      "0        1.610880       1.547568      1.594822        1.563308     1.579065   \n",
      "1        1.915039       1.839821      1.895975        1.858508     1.877242   \n",
      "2        1.166831       1.121043      1.155236        1.132408     1.143822   \n",
      "3        1.160835       1.115428      1.149526        1.126511     1.138018   \n",
      "4        1.062992       1.020975      1.052623        1.031133     1.041878   \n",
      "...           ...            ...           ...             ...          ...   \n",
      "8832     1.044199       1.003171      1.033933        1.013232     1.016325   \n",
      "8833     0.881647       0.846813      0.872976        0.855309     0.858015   \n",
      "8834     0.967895       0.925231      0.956187        0.936702     0.928908   \n",
      "8835     1.003179       0.963881      0.993225        0.973639     0.965210   \n",
      "8836     1.011310       0.971782      1.001321        0.981575     1.000093   \n",
      "\n",
      "      label  trainingWeight  EventWeight_cscReweight  massPoint  \n",
      "0         0        1.579065                 1.259129        NaN  \n",
      "1         0        1.877242                 1.745842        NaN  \n",
      "2         0        1.143822                 1.063759        NaN  \n",
      "3         0        1.138018                 1.058362        NaN  \n",
      "4         0        1.041878                 0.865819        NaN  \n",
      "...     ...             ...                      ...        ...  \n",
      "8832      1        2.181741                 1.023582        0p4  \n",
      "8833      1        1.841899                 0.864143        0p4  \n",
      "8834      1        2.017324                 0.946445        0p4  \n",
      "8835      1        2.096162                 0.983432        0p4  \n",
      "8836      1        2.113247                 0.991448        0p4  \n",
      "\n",
      "[360943 rows x 44 columns]]\n",
      "CHECKING WEIGHT VALUES\n",
      "             pt       eta       phi     staDR    staPhi        staE    staChi  \\\n",
      "0     33.490278  1.717840 -2.970404  0.005591 -0.005388   91.913979  0.400194   \n",
      "1     45.611144  2.120171 -1.608827  0.092025  0.091958   38.020702  1.457658   \n",
      "2     57.949479 -1.946880  1.466549  0.238449  0.232412   31.830992  0.463997   \n",
      "3     48.926747  2.264881 -1.678836  0.287264  0.287261   15.313004  2.503192   \n",
      "4     31.692128  1.646233  2.919638  0.247288  0.233991   14.703446  1.055910   \n",
      "...         ...       ...       ...       ...       ...         ...       ...   \n",
      "3150  42.043259  1.979610  1.894294  0.008176  0.007181  171.542648  0.771156   \n",
      "3151  46.627070 -1.831358  2.602722  0.006609  0.006402  134.220779  0.479760   \n",
      "3152  25.441795  1.553023  0.558573  0.139897 -0.133827   24.163887  1.256369   \n",
      "3153  35.071095  1.888268  0.813350  0.216143 -0.216014   18.459581  3.552273   \n",
      "3154  45.588241  2.267202 -2.868251  0.499868  0.489192   15.006203  1.886375   \n",
      "\n",
      "         cscDR  HEDepth_0  HEDepth_1  ...  IDdownWeight  ISOupWeight  \\\n",
      "0     0.004199   0.357786   0.597118  ...      0.949436     0.978540   \n",
      "1     0.004749   0.354347   0.923864  ...      1.048798     1.080853   \n",
      "2     0.019189   1.144569   0.791070  ...      1.042546     1.074410   \n",
      "3     0.007731   1.136549   1.562851  ...      0.864218     0.890692   \n",
      "4     0.037670   0.139668   0.766190  ...      0.947598     0.976645   \n",
      "...        ...        ...        ...  ...           ...          ...   \n",
      "3150  0.001267   0.340856   0.284907  ...      1.002647     1.033264   \n",
      "3151  0.003422   0.608746   0.708325  ...      1.002473     1.033085   \n",
      "3152  0.010429   0.000000   0.312680  ...      0.959964     0.990016   \n",
      "3153  0.017465   0.200268   0.788541  ...      0.951911     0.981065   \n",
      "3154  0.015367   1.296408   1.306509  ...      0.969153     0.998702   \n",
      "\n",
      "      ISOdownWeight  TrigUpWeight  TrigDownWeight  EnBinWeight  label  \\\n",
      "0          0.939861      0.968908        0.949298     0.949989      1   \n",
      "1          1.038485      1.070136        1.048990     1.049495      1   \n",
      "2          1.032182      1.063700        1.042680     1.059043      1   \n",
      "3          0.855929      0.882054        0.864394     0.882650      1   \n",
      "4          0.938041      0.967031        0.947460     0.987964      1   \n",
      "...             ...           ...             ...          ...    ...   \n",
      "3150       0.992698      1.022958        1.002801     0.994714      1   \n",
      "3151       0.992526      1.022781        1.002627     1.006386      1   \n",
      "3152       0.949682      0.979643        0.959845     0.982422      1   \n",
      "3153       0.942536      0.971366        0.952042     0.944596      1   \n",
      "3154       0.959580      0.988785        0.969301     0.947952      1   \n",
      "\n",
      "      trainingWeight  EventWeight_cscReweight  massPoint  \n",
      "0           5.719016                 0.959103        0p8  \n",
      "1           6.318047                 1.059563        0p8  \n",
      "2           6.280047                 1.053190        0p8  \n",
      "3           5.206932                 0.873224        0p8  \n",
      "4           5.707941                 0.957246        0p8  \n",
      "...              ...                      ...        ...  \n",
      "3150        6.039678                 1.012879        0p8  \n",
      "3151        6.038631                 1.012704        0p8  \n",
      "3152        5.782468                 0.969744        0p8  \n",
      "3153        5.734524                 0.961704        0p8  \n",
      "3154        5.837918                 0.979043        0p8  \n",
      "\n",
      "[3155 rows x 44 columns]\n",
      "0       0.959103\n",
      "1       1.059563\n",
      "2       1.053190\n",
      "3       0.873224\n",
      "4       0.957246\n",
      "          ...   \n",
      "3150    1.012879\n",
      "3151    1.012704\n",
      "3152    0.969744\n",
      "3153    0.961704\n",
      "3154    0.979043\n",
      "Name: EventWeight, Length: 3155, dtype: float64\n",
      "1767     0.0\n",
      "5818     0.0\n",
      "7611     0.0\n",
      "10759    0.0\n",
      "11455    0.0\n",
      "        ... \n",
      "4372     0.0\n",
      "4939     0.0\n",
      "5246     0.0\n",
      "5464     0.0\n",
      "6562     0.0\n",
      "Name: HEDepth_0, Length: 190, dtype: float64\n",
      "1767    -1.0\n",
      "5818    -1.0\n",
      "7611    -1.0\n",
      "10759   -1.0\n",
      "11455   -1.0\n",
      "        ... \n",
      "4372    -1.0\n",
      "4939    -1.0\n",
      "5246    -1.0\n",
      "5464    -1.0\n",
      "6562    -1.0\n",
      "Name: HEDepth_0, Length: 190, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "weightedInputs = []\n",
    "weightedInputsIndividualSignals = []\n",
    "for sigInput in sigFiles:\n",
    "   inputs.append(pd.concat([mcInputs,sigInput]))\n",
    "   weightedInputsIndividualSignals.append(pd.concat([weightedMcInputs,sigInput]))\n",
    "allSignals['EventWeight_cscReweight']=allSignals['EventWeight']\n",
    "allSignals_ecalBrem['EventWeight_cscReweight']=allSignals_ecalBrem['EventWeight']\n",
    "print(allSignals)\n",
    "weightedInputs.append(pd.concat([weightedMcInputs,allSignals]))\n",
    "print(weightedInputs)\n",
    "for dataset in weightedInputs:\n",
    "   print(\"CHECKING WEIGHT VALUES\")\n",
    "   print(dataset[dataset['massPoint']=='0p8'])\n",
    "   print(dataset[dataset['massPoint']=='0p8']['EventWeight'])\n",
    "for dataset in weightedInputs:\n",
    "   dataset['staPhi']=dataset['staPhi']*dataset['probeCharge']\n",
    "   print(dataset.loc[dataset['found_HEDepth_0']==0,'HEDepth_0']) \n",
    "   dataset.loc[dataset['found_HEDepth_0']==0,'HEDepth_0'] = dataset.loc[dataset['found_HEDepth_0']==0,'HEDepth_0'].replace(0,-1)\n",
    "   dataset.loc[dataset['found_HEDepth_1']==0,'HEDepth_1'] = dataset.loc[dataset['found_HEDepth_1']==0,'HEDepth_1'].replace(0,-1)\n",
    "   dataset.loc[dataset['found_HEDepth_2']==0,'HEDepth_2'] = dataset.loc[dataset['found_HEDepth_2']==0,'HEDepth_2'].replace(0,-1)\n",
    "   dataset.loc[dataset['found_HEDepth_3']==0,'HEDepth_3'] = dataset.loc[dataset['found_HEDepth_3']==0,'HEDepth_3'].replace(0,-1)\n",
    "   dataset.loc[dataset['found_HEDepth_4']==0,'HEDepth_4'] = dataset.loc[dataset['found_HEDepth_4']==0,'HEDepth_4'].replace(0,-1)\n",
    "   dataset.loc[dataset['found_HEDepth_5']==0,'HEDepth_5'] = dataset.loc[dataset['found_HEDepth_5']==0,'HEDepth_5'].replace(0,-1)\n",
    "   dataset.loc[dataset['found_HEDepth_6']==0,'HEDepth_6'] = dataset.loc[dataset['found_HEDepth_6']==0,'HEDepth_6'].replace(0,-1)\n",
    "   print(dataset.loc[dataset['found_HEDepth_0']==0,'HEDepth_0']) \n",
    "for sigFile in sigFiles:\n",
    "   sigFile['staPhi']=sigFile['staPhi']*sigFile['probeCharge']\n",
    "   sigFile.loc[sigFile['found_HEDepth_0']==0,'HEDepth_0'] = sigFile.loc[sigFile['found_HEDepth_0']==0,'HEDepth_0'].replace(0,-1)\n",
    "   sigFile.loc[sigFile['found_HEDepth_1']==0,'HEDepth_1'] = sigFile.loc[sigFile['found_HEDepth_1']==0,'HEDepth_1'].replace(0,-1)\n",
    "   sigFile.loc[sigFile['found_HEDepth_2']==0,'HEDepth_2'] = sigFile.loc[sigFile['found_HEDepth_2']==0,'HEDepth_2'].replace(0,-1)\n",
    "   sigFile.loc[sigFile['found_HEDepth_3']==0,'HEDepth_3'] = sigFile.loc[sigFile['found_HEDepth_3']==0,'HEDepth_3'].replace(0,-1)\n",
    "   sigFile.loc[sigFile['found_HEDepth_4']==0,'HEDepth_4'] = sigFile.loc[sigFile['found_HEDepth_4']==0,'HEDepth_4'].replace(0,-1)\n",
    "   sigFile.loc[sigFile['found_HEDepth_5']==0,'HEDepth_5'] = sigFile.loc[sigFile['found_HEDepth_5']==0,'HEDepth_5'].replace(0,-1)\n",
    "   sigFile.loc[sigFile['found_HEDepth_6']==0,'HEDepth_6'] = sigFile.loc[sigFile['found_HEDepth_6']==0,'HEDepth_6'].replace(0,-1)\n",
    "allSignals_ecalBrem['staPhi']=allSignals_ecalBrem['staPhi']*allSignals_ecalBrem['probeCharge']\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_0']==0,'HEDepth_0'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_0']==0,'HEDepth_0'].replace(0,-1)\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_1']==0,'HEDepth_1'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_1']==0,'HEDepth_1'].replace(0,-1)\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_2']==0,'HEDepth_2'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_2']==0,'HEDepth_2'].replace(0,-1)\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_3']==0,'HEDepth_3'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_3']==0,'HEDepth_3'].replace(0,-1)\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_4']==0,'HEDepth_4'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_4']==0,'HEDepth_4'].replace(0,-1)\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_5']==0,'HEDepth_5'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_5']==0,'HEDepth_5'].replace(0,-1)\n",
    "allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_6']==0,'HEDepth_6'] = allSignals_ecalBrem.loc[allSignals_ecalBrem['found_HEDepth_6']==0,'HEDepth_6'].replace(0,-1)    \n",
    "weightedMcInputs['staPhi']=weightedMcInputs['staPhi']*weightedMcInputs['probeCharge']\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_0']==0,'HEDepth_0'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_0']==0,'HEDepth_0'].replace(0,-1)\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_1']==0,'HEDepth_1'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_1']==0,'HEDepth_1'].replace(0,-1)\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_2']==0,'HEDepth_2'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_2']==0,'HEDepth_2'].replace(0,-1)\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_3']==0,'HEDepth_3'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_3']==0,'HEDepth_3'].replace(0,-1)\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_4']==0,'HEDepth_4'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_4']==0,'HEDepth_4'].replace(0,-1)\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_5']==0,'HEDepth_5'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_5']==0,'HEDepth_5'].replace(0,-1)\n",
    "weightedMcInputs.loc[weightedMcInputs['found_HEDepth_6']==0,'HEDepth_6'] = weightedMcInputs.loc[weightedMcInputs['found_HEDepth_6']==0,'HEDepth_6'].replace(0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22f8c783-35ec-41ba-ad8d-9eb21d381ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/auerb029/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "weightedMcInputs_SameSign = mcInputs_SameSign.copy()\n",
    "reweight=getHeDepthWeight(hcalSfHists[0],weightedMcInputs_SameSign['HEDepth_'+str(0)])\n",
    "for depth in range(1,7):\n",
    "   reweight = reweight*getHeDepthWeight(hcalSfHists[depth],weightedMcInputs_SameSign['HEDepth_'+str(depth)])\n",
    "weightedMcInputs_SameSign['EventWeight']=weightedMcInputs_SameSign['EventWeight']*reweight\n",
    "weightedMcInputs_SameSign['staPhi']=weightedMcInputs_SameSign['staPhi']*weightedMcInputs_SameSign['probeCharge']\n",
    "weightedMcInputs_SameSign['trainingWeight'] = weightedMcInputs_SameSign['EventWeight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ef40d7e-a253-4489-9223-dcfcbe40e315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/auerb029/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "weightedMCInputs_highHCAL = mcInputs_highHCAL.copy()\n",
    "reweight=getHeDepthWeight(hcalSfHists[0],weightedMCInputs_highHCAL['HEDepth_'+str(0)])\n",
    "for depth in range(1,7):\n",
    "   reweight = reweight*getHeDepthWeight(hcalSfHists[depth],weightedMCInputs_highHCAL['HEDepth_'+str(depth)])\n",
    "weightedMCInputs_highHCAL['EventWeight']=weightedMCInputs_highHCAL['EventWeight']*reweight\n",
    "weightedMCInputs_highHCAL['staPhi']=weightedMCInputs_highHCAL['staPhi']*weightedMCInputs_highHCAL['probeCharge']\n",
    "weightedMCInputs_highHCAL['trainingWeight'] = weightedMCInputs_highHCAL['EventWeight']\n",
    "\n",
    "dataInputs_highHCAL['staPhi']=dataInputs_highHCAL['staPhi']*dataInputs_highHCAL['probeCharge']\n",
    "dataInputs_highHCAL['trainingWeight'] = dataInputs_highHCAL['EventWeight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0665b-d776-4260-a830-5f8537735f68",
   "metadata": {},
   "source": [
    "This plots the DY and signal shapes of the various training variables prior to any BDT selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24c70956-6834-4758-a243-b1724517881c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacut = -0.6\n",
    "DYforPlotting = weightedMcInputs[(weightedMcInputs['standaloneDEoverE']<stacut) & (weightedMcInputs['cellEdgeDeta']>0.004) & (weightedMcInputs['cellEdgeDphi']>0.016)]# & ((weightedMcInputs['HEDepth_0']==0) | (weightedMcInputs['HEDepth_1']==0) | (weightedMcInputs['HEDepth_2']==0) | (weightedMcInputs['HEDepth_3']==0) | (weightedMcInputs['HEDepth_4']==0) | (weightedMcInputs['HEDepth_5']==0) | (weightedMcInputs['HEDepth_6']==0))]\n",
    "\n",
    "sigFilesForPlotting = []\n",
    "for sigFile in sigFiles:\n",
    "    signalForPlotting = sigFile[(sigFile['standaloneDEoverE']<stacut) & (sigFile['cellEdgeDeta']>0.004) & (sigFile['cellEdgeDphi']>0.016)]# & ((sigFile['HEDepth_0']==0) | (sigFile['HEDepth_1']==0) | (sigFile['HEDepth_2']==0) | (sigFile['HEDepth_3']==0) | (sigFile['HEDepth_4']==0) | (sigFile['HEDepth_5']==0) | (sigFile['HEDepth_6']==0))]\n",
    "    sigFilesForPlotting.append(signalForPlotting)\n",
    "\n",
    "### UNCOMMENT FOR PRODUCING PRE-BDT plots ###    \n",
    "#plotVariables(sigFilesForPlotting, sigMasses, DYforPlotting, allTrainingFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff00387-f54d-4a4f-8cbe-c5d621d82183",
   "metadata": {},
   "source": [
    "THIS FUNCTION IS DEPRECATED. It was used to plot the BDT distributions of the training and testing datasets before we moved to k-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffb705c2-fc56-48a3-80f5-1584397d32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotOverTraining(y_preds,labels,weight,y_predsTrain,labelsTrain,weightTrain,BDTname):\n",
    "  predAlt_labels = pd.DataFrame({'BDTscore':y_preds,'isSignal':labels,'eventWeights':weight})\n",
    "  predAltTrain_labels = pd.DataFrame({'BDTscore':y_predsTrain,'isSignal':labelsTrain,\n",
    "                                      'eventWeights':weightTrain})\n",
    "    \n",
    "  testSignalWeights = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==1,'eventWeights']\n",
    "  testSignalWeights_unity = testSignalWeights/(testSignalWeights.sum()*0.04)\n",
    "\n",
    "  testBkgWeights = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,'eventWeights']  \n",
    "  testBkgWeights_unity = testBkgWeights/(testBkgWeights.sum()*0.04)\n",
    "\n",
    "  trainSignalWeights = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==1,'eventWeights']\n",
    "  trainSignalWeights_unity = trainSignalWeights/(trainSignalWeights.sum()*0.04)\n",
    "\n",
    "  trainBkgWeights = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==0,'eventWeights']      \n",
    "  trainBkgWeights_unity = trainBkgWeights/(trainBkgWeights.sum()*0.04)\n",
    "\n",
    "  axes = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==1,:].hist(column='BDTscore',log=True,\n",
    "                                                                                  bins=np.arange(0,1.01,0.04),weights=trainSignalWeights_unity,\n",
    "                                                                                  label='Signal')# (train sample)')\n",
    "  predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Red\",weights=trainBkgWeights_unity,\n",
    "                                                                                 label='Background')# (train sample)')  \n",
    "    \n",
    "\n",
    "  output = pd.cut(predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,'BDTscore'],bins=np.arange(0,1.01,0.04))\n",
    "  print(output.value_counts())\n",
    "        \n",
    "  bin_edges = np.arange(0,1.01,0.04)         \n",
    "  bin_centers = (bin_edges[:-1] + bin_edges[1:])/2.  \n",
    "  \n",
    "  npBkg_predAlt_labels = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,:].to_numpy()\n",
    "  counts_bkg, binedges = np.histogram(npBkg_predAlt_labels[:,0], bins=bin_edges,weights=testBkgWeights_unity)\n",
    "  np_counts_bkg = np.asarray(counts_bkg)\n",
    "    \n",
    "  counts_bkg_sqrt = []\n",
    "  for bin_index in range(len(binedges) - 1):  \n",
    "    bin_left = binedges[bin_index]\n",
    "    bin_right = binedges[bin_index + 1]\n",
    "    in_bin = np.logical_and(bin_left < npBkg_predAlt_labels[:,0], npBkg_predAlt_labels[:,0] <= bin_right)\n",
    "\n",
    "    # filter the weights to only those inside the bin\n",
    "    weights_in_bin = testBkgWeights_unity[in_bin]\n",
    "\n",
    "    # compute the error however you want\n",
    "    error = np.sqrt(np.sum(weights_in_bin ** 2))\n",
    "    counts_bkg_sqrt.append(error)\n",
    "\n",
    "\n",
    "  npSignal_predAlt_labels = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==1,:].to_numpy()\n",
    "  counts_sig, binedges = np.histogram(npSignal_predAlt_labels[:,0], bins=bin_edges,weights=testSignalWeights_unity)\n",
    "  np_counts_sig = np.asarray(counts_sig)\n",
    "  counts_sig_sqrt = np.sqrt(np_counts_sig)\n",
    "\n",
    "  counts_sig_sqrt = []\n",
    "  for bin_index in range(len(binedges) - 1):  \n",
    "    bin_left = binedges[bin_index]\n",
    "    bin_right = binedges[bin_index + 1]\n",
    "    in_bin = np.logical_and(bin_left < npSignal_predAlt_labels[:,0], npSignal_predAlt_labels[:,0] <= bin_right)\n",
    "\n",
    "    # filter the weights to only those inside the bin\n",
    "    weights_in_bin = testSignalWeights_unity[in_bin]\n",
    "\n",
    "    # compute the error however you want\n",
    "    error = np.sqrt(np.sum(weights_in_bin ** 2))\n",
    "    counts_sig_sqrt.append(error)\n",
    "\n",
    "  plt.xlabel(\"BDT score\", loc='right')\n",
    "  plt.ylabel(\"1/N dN/dx\", loc='top')\n",
    "  plt.ylim([0.0004,40])\n",
    "  plt.legend()\n",
    "  plt.grid(False)    \n",
    "  plt.title(\" \")\n",
    "  plt.savefig(\"BDT_overTraining_\"+BDTname+\".pdf\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de5916-801e-4993-9397-a1b3c17535f2",
   "metadata": {},
   "source": [
    "This combines the datasets from the various k-folds before plotting the BDT distributions of the training vs testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "002b4292-5e26-4a88-860c-6fa882d9c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotOverTraining_kFold(y_preds,labels,weight,y_predsTrain,labelsTrain,weightTrain,BDTname):\n",
    "  array_predAltTrain_labels = []  \n",
    "  trainSignalWeights_unity = []\n",
    "  trainBkgWeights_unity = []\n",
    "\n",
    "  predAlt_labels = pd.DataFrame({'BDTscore':y_preds,'isSignal':labels,'eventWeights':weight})\n",
    "  for i in range(0,len(y_predsTrain)):\n",
    "    predAltTrain_labels =  pd.DataFrame({'BDTscore':y_predsTrain[i],'isSignal':labelsTrain[i],\n",
    "                                      'eventWeights':weightTrain[i]})\n",
    "    \n",
    "    array_predAltTrain_labels.append(predAltTrain_labels)\n",
    "      \n",
    "    trainSignalWeights = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==1,'eventWeights']\n",
    "    trainSignalWeights_unity.append(trainSignalWeights/(trainSignalWeights.sum()*0.04))\n",
    "\n",
    "    trainBkgWeights = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==0,'eventWeights']      \n",
    "    trainBkgWeights_unity.append(trainBkgWeights/(trainBkgWeights.sum()*0.04))\n",
    "    \n",
    "  testSignalWeights = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==1,'eventWeights']\n",
    "  testSignalWeights_unity = testSignalWeights/(testSignalWeights.sum()*0.04)\n",
    "\n",
    "  testBkgWeights = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,'eventWeights']  \n",
    "  testBkgWeights_unity = testBkgWeights/(testBkgWeights.sum()*0.04)\n",
    "\n",
    "  axes = array_predAltTrain_labels[0].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==1,:].hist(column='BDTscore',log=True,\n",
    "                                                                                  bins=np.arange(0,1.01,0.04),weights=trainSignalWeights_unity[0],\n",
    "                                                                                  label='Signal (train sample)')\n",
    "  array_predAltTrain_labels[0].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Red\",weights=trainBkgWeights_unity[0],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "    \n",
    "  array_predAltTrain_labels[1].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Green\",weights=trainBkgWeights_unity[1],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "\n",
    "  array_predAltTrain_labels[2].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Yellow\",weights=trainBkgWeights_unity[2],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "  array_predAltTrain_labels[3].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Orange\",weights=trainBkgWeights_unity[3],\n",
    "                                                                                 label='Bkg (train sample)')      \n",
    "  array_predAltTrain_labels[4].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Purple\",weights=trainBkgWeights_unity[4],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "    \n",
    "  output = pd.cut(predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,'BDTscore'],bins=np.arange(0,1.01,0.04))\n",
    "  print(output.value_counts())\n",
    "        \n",
    "    \n",
    "  bin_edges = np.arange(0,1.01,0.04)         \n",
    "  bin_centers = (bin_edges[:-1] + bin_edges[1:])/2.  \n",
    "  #axes.errorbars(bin_centers, )\n",
    "  \n",
    "  npBkg_predAlt_labels = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,:].to_numpy()\n",
    "  counts_bkg, binedges = np.histogram(npBkg_predAlt_labels[:,0], bins=bin_edges,weights=testBkgWeights_unity)\n",
    "  np_counts_bkg = np.asarray(counts_bkg)\n",
    "  #counts_bkg_sqrt = np.sqrt(np_counts_bkg)/(testBkgWeights.sum()*0.04)\n",
    "    \n",
    "  counts_bkg_sqrt = []\n",
    "  for bin_index in range(len(binedges) - 1):  \n",
    "    bin_left = binedges[bin_index]\n",
    "    bin_right = binedges[bin_index + 1]\n",
    "    in_bin = np.logical_and(bin_left < npBkg_predAlt_labels[:,0], npBkg_predAlt_labels[:,0] <= bin_right)\n",
    "\n",
    "    # filter the weights to only those inside the bin\n",
    "    weights_in_bin = testBkgWeights_unity[in_bin]\n",
    "\n",
    "    # compute the error however you want\n",
    "    error = np.sqrt(np.sum(weights_in_bin ** 2))\n",
    "    counts_bkg_sqrt.append(error)\n",
    "\n",
    "\n",
    "\n",
    "  plt.errorbar(bin_centers, counts_bkg, yerr=counts_bkg_sqrt, fmt='o', ecolor='Red',\n",
    "               mfc='Red',mec='Red',label='Background (test sample)')\n",
    "\n",
    "  npSignal_predAlt_labels = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==1,:].to_numpy()\n",
    "  counts_sig, binedges = np.histogram(npSignal_predAlt_labels[:,0], bins=bin_edges,weights=testSignalWeights_unity)\n",
    "  np_counts_sig = np.asarray(counts_sig)\n",
    "  counts_sig_sqrt = np.sqrt(np_counts_sig)\n",
    "#  counts_sig_sqrt = np.sqrt(np_counts_sig)/(testSignalWeights.sum()*0.04)\n",
    "\n",
    "  counts_sig_sqrt = []\n",
    "  for bin_index in range(len(binedges) - 1):  \n",
    "    bin_left = binedges[bin_index]\n",
    "    bin_right = binedges[bin_index + 1]\n",
    "    in_bin = np.logical_and(bin_left < npSignal_predAlt_labels[:,0], npSignal_predAlt_labels[:,0] <= bin_right)\n",
    "\n",
    "    # filter the weights to only those inside the bin\n",
    "    weights_in_bin = testSignalWeights_unity[in_bin]\n",
    "\n",
    "    # compute the error however you want\n",
    "    error = np.sqrt(np.sum(weights_in_bin ** 2))\n",
    "    counts_sig_sqrt.append(error)\n",
    "    \n",
    "  plt.errorbar(bin_centers, counts_sig, yerr=counts_sig_sqrt, fmt='o', ecolor='Blue',\n",
    "               mfc='Blue',mec='Blue', label='Signal (test sample)')\n",
    "\n",
    "  plt.xlabel(\"BDT score\", loc='right')\n",
    "  plt.ylabel(\"1/N dN/dx\", loc='top')\n",
    "  plt.ylim([0.0004,10000])\n",
    "  #plt.legend(ncol=2)\n",
    "  plt.legend(loc='upper center',ncol=2)\n",
    "  plt.grid(False)    \n",
    "  plt.title(\" \")\n",
    "  plt.savefig(\"BDT_overTraining_\"+BDTname+\".pdf\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1400d9-0ccb-4b6a-9f21-3add8bc0a280",
   "metadata": {},
   "source": [
    "This keeps the k-folds separate and plots the BDT distributions of the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "766e32d8-b349-4992-82d2-1a516629e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotOverTraining_kFold_individaulTraining(y_preds,labels,weight,y_predsTrain,labelsTrain,weightTrain,BDTname):\n",
    "  array_predAltTrain_labels = []  \n",
    "  trainSignalWeights_unity = []\n",
    "  trainBkgWeights_unity = []\n",
    "  array_predAlt_labels = []  \n",
    "  testSignalWeights_unity = []\n",
    "  testBkgWeights_unity = []\n",
    "    \n",
    "  output = []\n",
    "  #npBkg_predAlt_labels = []\n",
    "  array_counts_bkg = []\n",
    "  array_counts_bkg_sqrt = []\n",
    "  array_counts_bkgTrain = []    \n",
    "  bin_edges = np.arange(0,1.01,0.04)         \n",
    "  bin_centers = (bin_edges[:-1] + bin_edges[1:])/2.      \n",
    "\n",
    "  for i in range(0,len(y_predsTrain)):\n",
    "    predAltTrain_labels =  pd.DataFrame({'BDTscore':y_predsTrain[i],'isSignal':labelsTrain[i],\n",
    "                                      'eventWeights':weightTrain[i]})\n",
    "    predAlt_labels = pd.DataFrame({'BDTscore':y_preds[i],'isSignal':labels[i],'eventWeights':weight[i]})\n",
    "    \n",
    "    array_predAltTrain_labels.append(predAltTrain_labels)\n",
    "    array_predAlt_labels.append(predAlt_labels)\n",
    "      \n",
    "    trainSignalWeights = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==1,'eventWeights']\n",
    "    trainSignalWeights_unity.append(trainSignalWeights)\n",
    "#    trainSignalWeights_unity.append(trainSignalWeights/(trainSignalWeights.sum()*0.04))\n",
    "\n",
    "    trainBkgWeights = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==0,'eventWeights']      \n",
    "    trainBkgWeights_unity.append(trainBkgWeights)\n",
    "#    trainBkgWeights_unity.append(trainBkgWeights/(trainBkgWeights.sum()*0.04))\n",
    "    \n",
    "    print(\"trainBkgWeights.sum()\")\n",
    "    print(trainBkgWeights.sum())\n",
    "    \n",
    "    testSignalWeights = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==1,'eventWeights']\n",
    "    testSignalWeights_unity.append(testSignalWeights)\n",
    "#    testSignalWeights_unity.append(testSignalWeights/(testSignalWeights.sum()*0.04))\n",
    "\n",
    "    testBkgWeights = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,'eventWeights']  \n",
    "    testBkgWeights_unity.append(testBkgWeights)\n",
    "#    testBkgWeights_unity.append(testBkgWeights/(testBkgWeights.sum()*0.04))\n",
    "    \n",
    "    print(\"testBkgWeights.sum()\")\n",
    "    print(testBkgWeights.sum())\n",
    "    \n",
    "    output.append(pd.cut(predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,'BDTscore'],bins=np.arange(0,1.01,0.04)))\n",
    "\n",
    "    npBkg_predAlt_labels = predAlt_labels.loc[lambda predAlt_labels: predAlt_labels['isSignal']==0,:].to_numpy()\n",
    "    \n",
    "    counts_bkg, binedges = np.histogram(npBkg_predAlt_labels[:,0], bins=bin_edges,weights=testBkgWeights_unity[i])\n",
    "    array_counts_bkg.append(counts_bkg)\n",
    "    np_counts_bkg = np.asarray(counts_bkg)\n",
    "\n",
    "    npBkgTrain_predAlt_labels = predAltTrain_labels.loc[lambda predAltTrain_labels: predAltTrain_labels['isSignal']==0,:].to_numpy()\n",
    "    \n",
    "    counts_bkgTrain, binedges = np.histogram(npBkgTrain_predAlt_labels[:,0], bins=bin_edges,weights=trainBkgWeights_unity[i])\n",
    "    array_counts_bkgTrain.append(counts_bkgTrain)\n",
    "    \n",
    "    counts_bkg_sqrt = []\n",
    "    for bin_index in range(len(binedges) - 1):  \n",
    "         bin_left = binedges[bin_index]\n",
    "         bin_right = binedges[bin_index + 1]\n",
    "         in_bin = np.logical_and(bin_left < npBkg_predAlt_labels[:,0], npBkg_predAlt_labels[:,0] <= bin_right)\n",
    "\n",
    "         # filter the weights to only those inside the bin\n",
    "         weights_in_bin = testBkgWeights_unity[i][in_bin]\n",
    "\n",
    "         # compute the error however you want\n",
    "         error = np.sqrt(np.sum(weights_in_bin ** 2))\n",
    "         counts_bkg_sqrt.append(error)\n",
    "\n",
    "    array_counts_bkg_sqrt.append(counts_bkg_sqrt)\n",
    "\n",
    "  axes = array_predAltTrain_labels[0].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==1,:].hist(column='BDTscore',log=True,\n",
    "                                                                                  bins=np.arange(0,1.01,0.04),weights=trainSignalWeights_unity[0],\n",
    "                                                                                  label='Signal (train sample)')\n",
    "  array_predAltTrain_labels[0].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Red\",weights=trainBkgWeights_unity[0],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "    \n",
    "  array_predAltTrain_labels[1].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Green\",weights=trainBkgWeights_unity[1],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "\n",
    "  array_predAltTrain_labels[2].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Yellow\",weights=trainBkgWeights_unity[2],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "  array_predAltTrain_labels[3].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Orange\",weights=trainBkgWeights_unity[3],\n",
    "                                                                                 label='Bkg (train sample)')      \n",
    "  array_predAltTrain_labels[4].loc[lambda array_predAltTrain_labels: array_predAltTrain_labels['isSignal']==0,:].hist(column='BDTscore',bins=np.arange(0,1.01,0.04),\n",
    "                                                                                 ax=axes,histtype='step',hatch='/',\n",
    "                                                                                 edgecolor=\"Purple\",weights=trainBkgWeights_unity[4],\n",
    "                                                                                 label='Bkg (train sample)')  \n",
    "    \n",
    "  print(\"Test Sample bin counts:\")  \n",
    "  print(array_counts_bkg[0][0])\n",
    "  print(array_counts_bkg[0][4])    \n",
    "\n",
    "  print(\"Train Sample bin counts:\")  \n",
    "  print(array_counts_bkgTrain[0][0])\n",
    "  print(array_counts_bkgTrain[0][4])    \n",
    "\n",
    "  plt.errorbar(bin_centers, array_counts_bkg[0], yerr=array_counts_bkg_sqrt[0], fmt='o', ecolor='Red',\n",
    "               mfc='Red',mec='Red',label='Background (test sample)')\n",
    "\n",
    "  plt.errorbar(bin_centers, array_counts_bkg[1], yerr=array_counts_bkg_sqrt[1], fmt='o', ecolor='Green',\n",
    "               mfc='Green',mec='Green',label='Background (test sample)')\n",
    "    \n",
    "  plt.errorbar(bin_centers, array_counts_bkg[2], yerr=array_counts_bkg_sqrt[2], fmt='o', ecolor='Yellow',\n",
    "               mfc='Yellow',mec='Yellow',label='Background (test sample)')\n",
    "\n",
    "  plt.errorbar(bin_centers, array_counts_bkg[3], yerr=array_counts_bkg_sqrt[3], fmt='o', ecolor='Orange',\n",
    "               mfc='Orange',mec='Orange',label='Background (test sample)')\n",
    "    \n",
    "  plt.errorbar(bin_centers, array_counts_bkg[4], yerr=array_counts_bkg_sqrt[4], fmt='o', ecolor='Purple',\n",
    "               mfc='Purple',mec='Purple',label='Background (test sample)')\n",
    "\n",
    "  plt.xlabel(\"BDT score\", loc='right')\n",
    "  plt.ylabel(\"# of events\", loc='top')\n",
    "  #plt.ylim([0.0004,10000])\n",
    "  #plt.legend(ncol=2)\n",
    "  #plt.legend(loc='upper center',ncol=2)\n",
    "  #plt.grid(False)    \n",
    "  plt.title(\" \")\n",
    "  plt.savefig(\"BDT_overTraining_\"+BDTname+\"_allTestSamples.pdf\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d6e466-2468-4d35-a097-6f813386cc4e",
   "metadata": {},
   "source": [
    "This is the part where we start the bdt training. Specifically, this performs the optimization of the BDT structural parameters.\n",
    "\n",
    "First, a filter is applied to the data to select events with a standalone deltaE over E less than -0.6, so that only probes with standalone muons that have at least 60% less energy are used in order to increase the relative fraction of background events. Also, events that are not on the cell edge are selected. The efficiency of these cuts is calculated for DY and signal (mceff and sigeff) so that the ROC curves can be later re-scaled to the correct fractions. \n",
    "\n",
    "Then the dataframe is split into a dataframe that contains the training values and is used in the optimizing of the BDT structure. The other dataframe contains the various systematic weights and is not used until the end when we need to apply the different weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43e9d2-8b0e-4a61-9871-efa6a69924a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[             pt       eta       phi     staDR    staPhi        staE  \\\n",
      "0     36.234983  1.680514  0.178885  0.054614  0.048367   46.247898   \n",
      "1     48.588770 -1.953590  2.534030  0.054398  0.052517   59.415211   \n",
      "2     50.687919  2.220909  0.804715  0.068122 -0.068010  115.821770   \n",
      "3     49.187752  1.931597 -1.972739  0.021346 -0.021044  259.733459   \n",
      "4     37.035284 -2.366398  1.975624  0.096683  0.095750   44.757290   \n",
      "...         ...       ...       ...       ...       ...         ...   \n",
      "8832  81.194240  2.128157 -2.677567  0.027747  0.025751   54.011978   \n",
      "8833  40.267019  1.579129 -2.775568  0.688508  0.588957   14.518100   \n",
      "8834  43.994075 -1.905420  0.750612  0.032933  0.027979  226.703629   \n",
      "8835  41.336116  1.639106  1.828171  0.069064  0.035406  258.803253   \n",
      "8836  45.013371  2.131383  0.606705  0.071117  0.070803   60.930439   \n",
      "\n",
      "         staChi     cscDR  HEDepth_0  HEDepth_1  ...  IDdownWeight  \\\n",
      "0      7.220243  0.003537   0.297983   0.743317  ...      1.563022   \n",
      "1      0.753268  0.001801   0.226028   0.493166  ...      1.858269   \n",
      "2      1.541431  0.003291   0.258848   1.403674  ...      1.132262   \n",
      "3      1.857932  0.001489   0.190599   0.000000  ...      1.126282   \n",
      "4      3.293752  0.005710   0.869334   1.777585  ...      1.031290   \n",
      "...         ...       ...        ...        ...  ...           ...   \n",
      "8832   3.165587  0.014407   0.282289   0.511952  ...      1.013026   \n",
      "8833   2.853894  0.006740   0.171593   0.419994  ...      0.855433   \n",
      "8834  87.826294  0.004272   0.160230   0.802516  ...      0.936899   \n",
      "8835   1.413258  0.011116   0.184909   0.554984  ...      0.973260   \n",
      "8836   0.641197  0.004883   0.215138   0.848305  ...      0.981193   \n",
      "\n",
      "      ISOupWeight  ISOdownWeight  TrigUpWeight  TrigDownWeight  EnBinWeight  \\\n",
      "0        1.610880       1.547568      1.594822        1.563308     1.579065   \n",
      "1        1.915039       1.839821      1.895975        1.858508     1.877242   \n",
      "2        1.166831       1.121043      1.155236        1.132408     1.143822   \n",
      "3        1.160835       1.115428      1.149526        1.126511     1.138018   \n",
      "4        1.062992       1.020975      1.052623        1.031133     1.041878   \n",
      "...           ...            ...           ...             ...          ...   \n",
      "8832     1.044199       1.003171      1.033933        1.013232     1.016325   \n",
      "8833     0.881647       0.846813      0.872976        0.855309     0.858015   \n",
      "8834     0.967895       0.925231      0.956187        0.936702     0.928908   \n",
      "8835     1.003179       0.963881      0.993225        0.973639     0.965210   \n",
      "8836     1.011310       0.971782      1.001321        0.981575     1.000093   \n",
      "\n",
      "      label  trainingWeight  EventWeight_cscReweight  massPoint  \n",
      "0         0        1.579065                 1.259129        NaN  \n",
      "1         0        1.877242                 1.745842        NaN  \n",
      "2         0        1.143822                 1.063759        NaN  \n",
      "3         0        1.138018                 1.058362        NaN  \n",
      "4         0        1.041878                 0.865819        NaN  \n",
      "...     ...             ...                      ...        ...  \n",
      "8832      1        2.181741                 1.023582        0p4  \n",
      "8833      1        1.841899                 0.864143        0p4  \n",
      "8834      1        2.017324                 0.946445        0p4  \n",
      "8835      1        2.096162                 0.983432        0p4  \n",
      "8836      1        2.113247                 0.991448        0p4  \n",
      "\n",
      "[360943 rows x 44 columns]]\n",
      "['0p2', '1p0', '0p8', '0p6', '0p4']\n",
      "360943\n",
      "121178\n",
      "111858\n",
      "Number of total filtered data\n",
      "121178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/auerb029/.local/lib/python3.6/site-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number with no missing hits\n",
      "101163\n",
      "Number with one missing hit\n",
      "121178\n",
      "Number of backgrond with one missing hit\n",
      "111858\n",
      "CHECKING WEIGHT VALUES\n",
      "      EventWeight  PUupWeight  PUdownWeight  IDupWeight  IDdownWeight  \\\n",
      "1        1.002763    0.943537      1.051750    1.012897      0.992628   \n",
      "6        0.984855    0.903307      1.071040    0.994804      0.974906   \n",
      "7        0.967017    0.830561      1.108375    0.976786      0.957248   \n",
      "10       0.945212    0.811834      1.083383    0.954739      0.935685   \n",
      "15       1.031637    1.151504      0.843918    1.042059      1.021215   \n",
      "...           ...         ...           ...         ...           ...   \n",
      "8813     0.946963    0.859875      1.035454    0.956514      0.937413   \n",
      "8816     1.011301    1.003963      0.973781    1.021575      1.001026   \n",
      "8827     0.945737    0.872329      1.026241    0.955269      0.936205   \n",
      "8832     1.023582    1.142513      0.837329    1.034139      1.013026   \n",
      "8836     0.991448    0.919604      1.067274    1.001703      0.981193   \n",
      "\n",
      "      ISOupWeight  ISOdownWeight  TrigUpWeight  TrigDownWeight  EnBinWeight  \\\n",
      "1        1.022907       0.982819      1.012769        0.992756     1.025530   \n",
      "6        1.004630       0.965276      0.994655        0.975055     0.971157   \n",
      "7        0.986491       0.947736      0.976639        0.957394     1.010136   \n",
      "10       0.964287       0.926327      0.954875        0.935549     0.918078   \n",
      "15       1.052399       1.011081      1.041902        1.021372     1.037352   \n",
      "...           ...            ...           ...             ...          ...   \n",
      "8813     0.966057       0.928061      0.956630        0.937297     0.942951   \n",
      "8816     1.031667       0.991137      1.021392        1.001209     1.016903   \n",
      "8827     0.964903       0.926763      0.955405        0.936069     0.932300   \n",
      "8832     1.044199       1.003171      1.033933        1.013232     1.016325   \n",
      "8836     1.011310       0.971782      1.001321        0.981575     1.000093   \n",
      "\n",
      "      label  trainingWeight  EventWeight_cscReweight massPoint  \n",
      "1         1        2.137365                 1.002763       0p4  \n",
      "6         1        2.099194                 0.984855       0p4  \n",
      "7         1        2.061173                 0.967017       0p4  \n",
      "10        1        2.014697                 0.945212       0p4  \n",
      "15        1        2.198909                 1.031637       0p4  \n",
      "...     ...             ...                      ...       ...  \n",
      "8813      1        2.018430                 0.946963       0p4  \n",
      "8816      1        2.155563                 1.011301       0p4  \n",
      "8827      1        2.015816                 0.945737       0p4  \n",
      "8832      1        2.181741                 1.023582       0p4  \n",
      "8836      1        2.113247                 0.991448       0p4  \n",
      "\n",
      "[2268 rows x 14 columns]\n",
      "             pt       eta       phi     staDR    staPhi       staE     staChi  \\\n",
      "1     48.588770 -1.953590  2.534030  0.054398  0.052517  59.415211   0.753268   \n",
      "4     37.035284 -2.366398  1.975624  0.096683  0.095750  44.757290   3.293752   \n",
      "7     50.515547  1.671956 -0.568947  0.283308 -0.002418  25.153557  60.916972   \n",
      "8     44.155022 -2.124426  0.979609  0.405137  0.403815  11.643636   5.180820   \n",
      "9     80.522550 -2.381075  2.027694  0.114773 -0.114573  67.719337   4.629184   \n",
      "...         ...       ...       ...       ...       ...        ...        ...   \n",
      "8813  33.384541  1.798086  0.646761  0.113266  0.112714  29.977648   0.509440   \n",
      "8816  41.492357  1.840194  0.207319  0.238704  0.225085  10.987767   0.583223   \n",
      "8827  34.012573  1.876297  2.034083  0.706046  0.706002   6.221093   1.084464   \n",
      "8832  81.194240  2.128157 -2.677567  0.027747  0.025751  54.011978   3.165587   \n",
      "8836  45.013371  2.131383  0.606705  0.071117  0.070803  60.930439   0.641197   \n",
      "\n",
      "         cscDR  HEDepth_0  HEDepth_1  ...  probeCharge  standaloneDEoverE  \\\n",
      "1     0.001801   0.226028   0.493166  ...          1.0          -0.660127   \n",
      "4     0.005710   0.869334   1.777585  ...          1.0          -0.775219   \n",
      "7     0.006505   0.308177   0.821580  ...         -1.0          -0.819275   \n",
      "8     0.004045   0.166511   0.431028  ...          1.0          -0.937862   \n",
      "9     0.002713   0.398120   0.472251  ...         -1.0          -0.845815   \n",
      "...        ...        ...        ...  ...          ...                ...   \n",
      "8813  0.004772   0.695881   0.633948  ...         -1.0          -0.710511   \n",
      "8816  0.057664   0.201955   0.559785  ...         -1.0          -0.917970   \n",
      "8827  0.080369   0.204025   0.227207  ...         -1.0          -0.945258   \n",
      "8832  0.014407   0.282289   0.511952  ...          1.0          -0.843816   \n",
      "8836  0.004883   0.215138   0.848305  ...         -1.0          -0.683188   \n",
      "\n",
      "      cscDRbyStation_0  cscDRbyStation_1  cscDRbyStation_2  cscDRbyStation_3  \\\n",
      "1             0.010030          0.001801          0.009905          0.013251   \n",
      "4             0.021389          0.005710          0.008495          0.011281   \n",
      "7             0.012604          0.006505          0.011184          0.015411   \n",
      "8             4.416383          0.004045          0.005047          0.008676   \n",
      "9             0.006248          0.007338          0.002713          0.003895   \n",
      "...                ...               ...               ...               ...   \n",
      "8813          0.005270          0.007047          0.004772         -1.000000   \n",
      "8816          0.057664          0.111606          0.105912          0.092100   \n",
      "8827          0.080369          1.228865          1.249003         -1.000000   \n",
      "8832          0.014407          0.027165          0.027197          0.023879   \n",
      "8836          0.004883          0.005243          0.006980          0.010598   \n",
      "\n",
      "      EventWeight  label  trainingWeight  massPoint  \n",
      "1        1.877242      0        1.877242        NaN  \n",
      "4        1.041878      0        1.041878        NaN  \n",
      "7        3.931283      0        3.931283        NaN  \n",
      "8        0.558645      0        0.558645        NaN  \n",
      "9        1.034571      0        1.034571        NaN  \n",
      "...           ...    ...             ...        ...  \n",
      "8813     0.946963      1        2.018430        0p4  \n",
      "8816     1.011301      1        2.155563        0p4  \n",
      "8827     0.945737      1        2.015816        0p4  \n",
      "8832     1.023582      1        2.181741        0p4  \n",
      "8836     0.991448      1        2.113247        0p4  \n",
      "\n",
      "[121178 rows x 25 columns]\n",
      "['pt', 'eta', 'phi', 'staDR', 'staPhi', 'staE', 'staChi', 'cscDR', 'HEDepth_0', 'HEDepth_1', 'HEDepth_2', 'HEDepth_3', 'HEDepth_4', 'HEDepth_5', 'HEDepth_6', 'probeCharge', 'standaloneDEoverE', 'cscDRbyStation_0', 'cscDRbyStation_1', 'cscDRbyStation_2', 'cscDRbyStation_3', 'trainingWeight']\n",
      "['pt', 'eta', 'phi', 'staDR', 'staPhi', 'staE', 'staChi', 'cscDR', 'HEDepth_0', 'HEDepth_1', 'HEDepth_2', 'HEDepth_3', 'HEDepth_4', 'HEDepth_5', 'HEDepth_6', 'probeCharge', 'standaloneDEoverE', 'cscDRbyStation_0', 'cscDRbyStation_1', 'cscDRbyStation_2', 'cscDRbyStation_3']\n",
      "21\n",
      "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'top_k_accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])\n",
      "96942\n",
      "21\n",
      "RandomizedSearchCV(cv=5,\n",
      "                   estimator=XGBRegressor(base_score=None, booster='gbtree',\n",
      "                                          colsample_bylevel=None,\n",
      "                                          colsample_bynode=None,\n",
      "                                          colsample_bytree=None,\n",
      "                                          enable_categorical=False,\n",
      "                                          eval_metric='logloss', gamma=None,\n",
      "                                          gpu_id=None, importance_type=None,\n",
      "                                          interaction_constraints=None,\n",
      "                                          learning_rate=None,\n",
      "                                          max_delta_step=None, max_depth=None,\n",
      "                                          min_child_weight=None, missing...\n",
      "                                        'reg_alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe1b78620f0>,\n",
      "                                        'reg_lambda': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe1b9460cc0>,\n",
      "                                        'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe1beae1e10>},\n",
      "                   scoring=make_scorer(neg_mean_squared_weighted_error, weights=[0.00955564 0.56866266 0.5036373  ... 1.03652336 1.05171957 1.43130329]),\n",
      "                   verbose=1)\n",
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "weightedInputs2 = weightedInputs.copy()\n",
    "print(weightedInputs)\n",
    "print(sigMasses)\n",
    "\n",
    "#for idx, dataset in enumerate(halfWeightedInputs):\n",
    "for idx, dataset in enumerate(weightedInputs):\n",
    "  stacut = -0.6\n",
    "  print(len(dataset))\n",
    "  filter = (dataset['standaloneDEoverE']<stacut) & (dataset['cellEdgeDeta']>0.004) & (dataset['cellEdgeDphi']>0.016)\n",
    "  filteredData = dataset[filter]\n",
    "  print(len(filteredData))\n",
    "  bkg_filteredData = filteredData.copy()\n",
    "  bkg_filteredData = bkg_filteredData[bkg_filteredData['label']<0.5]\n",
    "  print(len(bkg_filteredData))\n",
    "    \n",
    "  filteredData.drop('cellEdgeDphi', inplace=True, axis=1)\n",
    "  filteredData.drop('cellEdgeDeta', inplace=True, axis=1)\n",
    "  filteredData.drop('found_HEDepth_0', inplace=True, axis=1)\n",
    "  filteredData.drop('found_HEDepth_1', inplace=True, axis=1)\n",
    "  filteredData.drop('found_HEDepth_2', inplace=True, axis=1)\n",
    "  filteredData.drop('found_HEDepth_3', inplace=True, axis=1)\n",
    "  filteredData.drop('found_HEDepth_4', inplace=True, axis=1)\n",
    "  filteredData.drop('found_HEDepth_5', inplace=True, axis=1)\n",
    "  filteredData.drop('found_HEDepth_6', inplace=True, axis=1)    \n",
    "  print(\"Number of total filtered data\")\n",
    "  print(len(filteredData))\n",
    "  haveAll = filteredData.copy()\n",
    "  missingOneHit = filteredData.copy()\n",
    "#  missingOneHit = missingOneHit[((missingOneHit['HEDepth_0']==0) | (missingOneHit['HEDepth_1']==0) | (missingOneHit['HEDepth_2']==0) | (missingOneHit['HEDepth_3']==0) | (missingOneHit['HEDepth_4']==0) | (missingOneHit['HEDepth_5']==0) | (missingOneHit['HEDepth_6']==0))]\n",
    "  for depth in range(7):\n",
    "    HEfilter = haveAll['HEDepth_'+str(depth)]!=0\n",
    "    haveAll = haveAll[HEfilter]\n",
    "    \n",
    "  print(\"Number with no missing hits\")\n",
    "  print(len(haveAll))\n",
    "  print(\"Number with one missing hit\")\n",
    "  print(len(missingOneHit))\n",
    "  print(\"Number of backgrond with one missing hit\")\n",
    "  bkg_missingOneHit = missingOneHit.copy()\n",
    "  bkg_missingOneHit = bkg_missingOneHit[bkg_missingOneHit['label']<0.5]\n",
    "  print(len(bkg_missingOneHit))\n",
    "    \n",
    "  missingOneHit_SystWeights = missingOneHit.copy()\n",
    "  missingOneHit_SystWeights.drop(['pt'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['eta'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['phi'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['staDR'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['staPhi'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['staE'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['staChi'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['cscDR'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['HEDepth_0'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['HEDepth_1'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['HEDepth_2'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['HEDepth_3'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['HEDepth_4'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['HEDepth_5'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['HEDepth_6'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['probeCharge'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['standaloneDEoverE'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['cscDRbyStation_0'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['cscDRbyStation_1'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['cscDRbyStation_2'],inplace=True, axis=1)\n",
    "  missingOneHit_SystWeights.drop(['cscDRbyStation_3'],inplace=True, axis=1)    \n",
    "  print(\"CHECKING WEIGHT VALUES\")\n",
    "  print(missingOneHit_SystWeights[missingOneHit_SystWeights['massPoint']=='0p4'])\n",
    "\n",
    "  missingOneHit.drop(['PUupWeight'],inplace=True, axis=1)    \n",
    "  missingOneHit.drop(['PUdownWeight'],inplace=True, axis=1)    \n",
    "  missingOneHit.drop(['IDupWeight'],inplace=True, axis=1)    \n",
    "  missingOneHit.drop(['IDdownWeight'],inplace=True, axis=1)    \n",
    "  missingOneHit.drop(['ISOupWeight'],inplace=True, axis=1)    \n",
    "  missingOneHit.drop(['ISOdownWeight'],inplace=True, axis=1)    \n",
    "  missingOneHit.drop(['TrigUpWeight'],inplace=True, axis=1)    \n",
    "  missingOneHit.drop(['TrigDownWeight'],inplace=True, axis=1)    \n",
    "  missingOneHit.drop(['EnBinWeight'],inplace=True, axis=1) \n",
    "  missingOneHit.drop(['EventWeight_cscReweight'],inplace=True, axis=1) \n",
    "  print(missingOneHit)\n",
    "    \n",
    "  mc = dataset[dataset['label']<0.5]\n",
    "  mc_filter = mc.copy()\n",
    "\n",
    "\n",
    "  mceff = mc_filter[(mc_filter['standaloneDEoverE']<stacut) & (mc_filter['cellEdgeDeta']>0.004) & (mc_filter['cellEdgeDphi']>0.016)].shape[0]/mc.shape[0]# & \n",
    "                   #((mc_filter['HEDepth_0']==0) | (mc_filter['HEDepth_1']==0) | (mc_filter['HEDepth_2']==0) | (mc_filter['HEDepth_3']==0) | (mc_filter['HEDepth_4']==0) | (mc_filter['HEDepth_5']==0) | (mc_filter['HEDepth_6']==0))].shape[0]/mc.shape[0]\n",
    "#                   ((mc_filter['HEDepth_0']==0) | (mc_filter['HEDepth_1']==0) | (mc_filter['HEDepth_2']==0) | (mc_filter['HEDepth_3']==0) | (mc_filter['HEDepth_4']==0) | (mc_filter['HEDepth_5']==0))].shape[0]/mc.shape[0]\n",
    "  signal = dataset[dataset['label']>0.5]\n",
    "  signal_filter = signal.copy()\n",
    "\n",
    "  sigeff = signal_filter[(signal_filter['standaloneDEoverE']<stacut) & (signal_filter['cellEdgeDeta']>0.004) & (signal_filter['cellEdgeDphi']>0.016)].shape[0]/signal.shape[0]# &\n",
    "                   #((signal_filter['HEDepth_0']==0) | (signal_filter['HEDepth_1']==0) | (signal_filter['HEDepth_2']==0) | (signal_filter['HEDepth_3']==0) | (signal_filter['HEDepth_4']==0) | (signal_filter['HEDepth_5']==0) | (signal_filter['HEDepth_6']==0))].shape[0]/signal.shape[0]  \n",
    "#                   ((signal_filter['HEDepth_0']==0) | (signal_filter['HEDepth_1']==0) | (signal_filter['HEDepth_2']==0) | (signal_filter['HEDepth_3']==0) | (signal_filter['HEDepth_4']==0) | (signal_filter['HEDepth_5']==0))].shape[0]/signal.shape[0]  \n",
    "\n",
    "  best_estimator, best_params, train, test, trainWeight, testWeight, raw_pos_weight, train_array, test_array, test_masses = optimize_hyper_xg(missingOneHit,allTrainingFeatures,1)\n",
    "#  best_estimator, best_params, train, test, trainWeight, testWeight, raw_pos_weight, train_array, test_array, test_masses = optimize_hyper_xg(filteredData,allTrainingFeatures,1)\n",
    "  print(\"FINISHED optimize_hyper_xg\")\n",
    "  print(\"raw_pos_weight\")\n",
    "  print(raw_pos_weight)\n",
    "\n",
    "  nFolds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5fb38-2517-457b-b83d-b8c65f7ab2ac",
   "metadata": {},
   "source": [
    "This performs the final BDT training through the k-fold validation. The various training datasets, testing datasets, and BDTs from the different folds are kept track of. Along with the event weights and labels of the various datasets. This information is then used to create arrays of the BDT predictions for all signal and DY events.\n",
    "\n",
    "And finally, the BDT distributions of the training and testing datasets are plotted along with the importance of the various training variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff610fe9-1fce-4122-a5ae-fc6aa02571b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "  curves = []\n",
    "  importance = []\n",
    "  sigPreds = []\n",
    "  newGenPreds = [] \n",
    "\n",
    "  bdtArray, array_train, array_test, array_train_array, array_test_array, array_train_lbl, array_test_lbl, array_trainWeight, array_testWeight, array_testMasses, array_SystWeights = kFoldCrossValidation(best_params,missingOneHit,allTrainingFeatures,nFolds,missingOneHit_SystWeights)\n",
    "#  bdtArray, array_train, array_test, array_train_array, array_test_array, array_train_lbl, array_test_lbl, array_trainWeight, array_testWeight, array_testMasses = kFoldCrossValidation(best_params,filteredData,allTrainingFeatures,nFolds)\n",
    "  print(len(bdtArray[0].evals_result()))\n",
    "  print(bdtArray[0].evals_result())\n",
    "  print(\"len(array_test_array)\")\n",
    "  print(len(array_test_array))\n",
    "\n",
    "  sumLogLoss = 0\n",
    "  for i in range(0,nFolds):\n",
    "    eval_results = bdtArray[i].evals_result()\n",
    "    sumLogLoss = sumLogLoss + eval_results['validation_0']['logloss'][len(eval_results['validation_0']['logloss'])-1]\n",
    "  meanLogLoss = sumLogLoss/nFolds\n",
    "  print(\"meanLogLoss\")\n",
    "  print(meanLogLoss)\n",
    "    \n",
    "  sumSquaredLogLoss = 0\n",
    "  for i in range(0,nFolds): \n",
    "    eval_results = bdtArray[i].evals_result()\n",
    "    sumSquaredLogLoss = sumSquaredLogLoss + (meanLogLoss - eval_results['validation_0']['logloss'][len(eval_results['validation_0']['logloss'])-1])**2\n",
    "    \n",
    "  stdLogLoss = mth.sqrt(sumSquaredLogLoss/nFolds)\n",
    "  print(\"stdLogLoss\")\n",
    "  print(stdLogLoss)\n",
    "\n",
    "  #print(eval_results['validation_0']['logloss'])\n",
    "  #print(eval_results['validation_0']['logloss'][len(eval_results['validation_0']['logloss'])-1])\n",
    "  #for key, value in eval_results['validation_0'].items():\n",
    "  #      print(key, value)\n",
    "\n",
    "  haveAll_x,haveAll_y,haveAll_masses = format_data(haveAll,allTrainingFeatures)\n",
    "  haveAll_weights = haveAll_x[:,haveAll_x.shape[1]-1]\n",
    "  haveAll_x = np.delete(haveAll_x,haveAll_x.shape[1]-1,1)            \n",
    "\n",
    "  haveAll_pred = []\n",
    "  for i in range(0,nFolds):\n",
    "    haveAll_pred.append(bdtArray[i].predict(haveAll_x))\n",
    "#    print(\"BDT i\")\n",
    "#    print (i)\n",
    "#    for j in range(0,len(haveAll_pred[i])):\n",
    "#        print(j)\n",
    "#        print(haveAll_pred[i][j])\n",
    "\n",
    "\n",
    "  print(\"Number of haveAll events\")\n",
    "  print(len(haveAll_pred[0]))\n",
    "  haveAll_y = haveAll_y.flatten()\n",
    "  print(haveAll_y)\n",
    "  print(haveAll_weights)\n",
    "  nBkg_haveAll = 0\n",
    "  for i in range(0,len(haveAll_pred[0])):\n",
    "    if haveAll_y[i] == 0: nBkg_haveAll = nBkg_haveAll + 1\n",
    "    \n",
    "  print(nBkg_haveAll)\n",
    "    \n",
    "  y_preds, labels, weights, masses_labels, testEvents_array, BDToutput_systWeights = combiningKFolds(bdtArray,array_test,array_test_array,array_testWeight,array_testMasses,array_train,array_train_array,array_trainWeight,nFolds, array_SystWeights)\n",
    "\n",
    "  print(len(BDToutput_systWeights))\n",
    "  print(\"DEBUG\")\n",
    "  print(len(testEvents_array))\n",
    "  print(len(y_preds))\n",
    "  print(len(weights))\n",
    "  print(len(labels))\n",
    "  nBkg_AllEvents = 0\n",
    "  for i in range(0,len(y_preds)):\n",
    "    if labels[i] == 0: nBkg_AllEvents = nBkg_AllEvents + 1\n",
    "    \n",
    "  print(\"Number of Bkg events total\")\n",
    "  print(nBkg_AllEvents)\n",
    "\n",
    "  fpralt, tpralt, thresholdsalt = roc_curve(labels, y_preds,sample_weight=weights) \n",
    "\n",
    "  print(\"DEBUGGING\")\n",
    "  print(fpralt)\n",
    "  print(len(fpralt))\n",
    "\n",
    "\n",
    "##### CHARGE NOT USED IN BDT ######\n",
    "#  filteredData.drop('probeCharge', inplace=True, axis=1)\n",
    "#  best_estimator_noC, best_params_noC, train_noC, test_noC, trainWeight_noC, testWeight_noC, raw_pos_weight_noC, train_array_noC, test_array_noC, test_masses_noC = optimize_hyper_xg(filteredData,allTrainingFeatures_noC,1)\n",
    "\n",
    "#  y_predsAlt_noC = best_estimator_noC.predict(test_array_noC)\n",
    "#  labelsalt_noC = test_noC.get_label()\n",
    "#  fpralt_noC, tpralt_noC, thresholdsalt_noC = roc_curve(labelsalt_noC, y_predsAlt_noC,sample_weight=testWeight_noC) \n",
    "\n",
    "#  y_predsAltTrain_noC = best_estimator_noC.predict(train_array_noC)\n",
    "#  labelsaltTrain_noC = train_noC.get_label()    \n",
    "#####################################\n",
    "         \n",
    "\n",
    "  #Using predictions of testing dataset\n",
    "  bkg_predsAlt = []\n",
    "  sig0p2_predsAlt = []\n",
    "  sig0p4_predsAlt = []\n",
    "  sig0p6_predsAlt = []\n",
    "  sig0p8_predsAlt = []\n",
    "  sig1p0_predsAlt = []\n",
    "#  sig2p0_predsAlt = []   \n",
    "  for idx2, BDTscore in enumerate(y_preds):\n",
    "      if labels[idx2] == 0: bkg_predsAlt.append(BDTscore)\n",
    "      else:\n",
    "            if masses_labels[idx2] == \"0p2\": sig0p2_predsAlt.append(BDTscore)\n",
    "            elif masses_labels[idx2] == \"0p4\": sig0p4_predsAlt.append(BDTscore)\n",
    "            elif masses_labels[idx2] == \"0p6\": sig0p6_predsAlt.append(BDTscore)\n",
    "            elif masses_labels[idx2] == \"0p8\": sig0p8_predsAlt.append(BDTscore)\n",
    "            elif masses_labels[idx2] == \"1p0\": sig1p0_predsAlt.append(BDTscore)\n",
    "#            elif masses_labels[idx2] == \"2p0\": sig2p0_predsAlt.append(BDTscore)\n",
    "\n",
    "###UNCOMMENT TO PLOT SIGNAL LIKE BACKGROUND EVENTS    \n",
    "  #plotVariables(sig_SignalLikeEvents, sigMasses, df_bkg_SignalLikeEvents, allTrainingFeatures)    \n",
    "\n",
    "  #plotOverTraining(y_predsAlt,labelsalt,array_testWeight[0],y_predsAltTrain,labelsaltTrain,array_trainWeight[0],\"allVariables\")\n",
    "  #plotOverTraining(y_predsAlt_noC,labelsalt_noC,testWeight_noC,y_predsAltTrain_noC,labelsaltTrain_noC,trainWeight_noC,\"noTrackCharge\")\n",
    "    \n",
    "\n",
    "  newGenPreds.append(bkg_predsAlt)\n",
    "  ###When using the fixBiasing samples###\n",
    "  sigPreds.append(sig0p4_predsAlt)\n",
    "  sigPreds.append(sig0p8_predsAlt)\n",
    "  sigPreds.append(sig0p6_predsAlt)\n",
    "  sigPreds.append(sig0p2_predsAlt)\n",
    "  sigPreds.append(sig1p0_predsAlt)\n",
    "  ###When using the ecalBrem samples###\n",
    "  #sigPreds.append(sig0p4_predsAlt)\n",
    "  #sigPreds.append(sig0p6_predsAlt)\n",
    "  #sigPreds.append(sig0p2_predsAlt)\n",
    "  #sigPreds.append(sig0p8_predsAlt)\n",
    "  #sigPreds.append(sig1p0_predsAlt)    \n",
    "\n",
    "#  importance.append(xgb.plot_importance(bst))\n",
    "  importance.append(xgb.plot_importance(bdtArray[0]))\n",
    "    \n",
    "  Text_yticklabels = list(importance[0].get_yticklabels())\n",
    "  dict_features = dict(enumerate(allTrainingFeatures[0:len(allTrainingFeatures)-1]))\n",
    "  lst_yticklabels = [ Text_yticklabels[i].get_text().lstrip('f') for i in range(len(Text_yticklabels))]\n",
    "  lst_yticklabels = [ dict_features[int(i)] for i in lst_yticklabels]\n",
    "\n",
    "  importance[0].set_yticklabels(lst_yticklabels)\n",
    "  print(dict_features)\n",
    "  plt.show()\n",
    "\n",
    "  #importance.append(xgb.plot_importance(best_estimator_noC))\n",
    "    \n",
    "  #Text_yticklabels = list(importance[1].get_yticklabels())\n",
    "  #dict_features = dict(enumerate(allTrainingFeatures_noC[0:len(allTrainingFeatures_noC)-1]))\n",
    "  #lst_yticklabels = [ Text_yticklabels[i].get_text().lstrip('f') for i in range(len(Text_yticklabels))]\n",
    "  #lst_yticklabels = [ dict_features[int(i)] for i in lst_yticklabels]\n",
    "\n",
    "  #importance[1].set_yticklabels(lst_yticklabels)\n",
    "  #print(dict_features)\n",
    "  #plt.show()\n",
    "\n",
    "  print(\"DEBUGGING\")\n",
    "  print(len(weights))\n",
    "\n",
    "  curves.append([fpralt,tpralt,fpralt,tpralt,mceff,sigeff,fpralt,tpralt,mceff,sigeff])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5962cc07-078f-4175-a749-6d8633cae383",
   "metadata": {},
   "source": [
    "This plots the training variables for the high BDT events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72887b9f-cdfc-4f2a-a66a-4c5ca9141233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plot the signal like events. These are selected in the above cell\n",
    "  bkg_SignalLikeEvents = []\n",
    "  sig0p2_SignalLikeEvents = []     \n",
    "  sig0p4_SignalLikeEvents = []     \n",
    "  sig0p6_SignalLikeEvents = []     \n",
    "  sig0p8_SignalLikeEvents = []     \n",
    "  sig1p0_SignalLikeEvents = []     \n",
    "#  sig2p0_SignalLikeEvents = []         \n",
    "  print(\"LOOKING FOR SIGNAL LIKE EVENTS\")    \n",
    "  print(len(masses_labels))\n",
    "  print(len(y_preds))\n",
    "  print(len(testEvents_array))\n",
    "  print(len(weights))\n",
    "  for idx2, BDTscore in enumerate(y_preds):\n",
    "      if BDTscore > 0.95 and labels[idx2]==0:\n",
    "            print(idx2)\n",
    "            print(BDTscore)\n",
    "            bkg_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    "      elif BDTscore > 0.95 and labels[idx2]==1:\n",
    "            if masses_labels[idx2] == \"0p2\": sig0p2_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    "            elif masses_labels[idx2] == \"0p4\": sig0p4_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    "            elif masses_labels[idx2] == \"0p6\": sig0p6_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    "            elif masses_labels[idx2] == \"0p8\": sig0p8_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    "            elif masses_labels[idx2] == \"1p0\": sig1p0_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    "#            elif masses_labels[idx2] == \"2p0\": sig2p0_SignalLikeEvents.append(np.concatenate((testEvents_array[idx2],[weights[idx2]])))\n",
    " \n",
    "  print(\"Number of background events\")\n",
    "  print(len(bkg_SignalLikeEvents))\n",
    "  print(len(sig0p8_SignalLikeEvents))\n",
    "  np_bkg_SignalLikeEvents = np.array(bkg_SignalLikeEvents)\n",
    "  np_sig0p2_SignalLikeEvents = np.array(sig0p2_SignalLikeEvents)\n",
    "  np_sig0p4_SignalLikeEvents = np.array(sig0p4_SignalLikeEvents)\n",
    "  np_sig0p6_SignalLikeEvents = np.array(sig0p6_SignalLikeEvents)\n",
    "  np_sig0p8_SignalLikeEvents = np.array(sig0p8_SignalLikeEvents)\n",
    "  np_sig1p0_SignalLikeEvents = np.array(sig1p0_SignalLikeEvents)\n",
    "#  np_sig2p0_SignalLikeEvents = np.array(sig2p0_SignalLikeEvents)\n",
    "  print(len(allTrainingFeatures))\n",
    "  print(np_sig0p8_SignalLikeEvents)\n",
    "  df_bkg_SignalLikeEvents = pd.DataFrame(np_bkg_SignalLikeEvents, columns=allTrainingFeatures)\n",
    "  df_sig0p2_SignalLikeEvents = pd.DataFrame(np_sig0p2_SignalLikeEvents, columns=allTrainingFeatures)\n",
    "  df_sig0p4_SignalLikeEvents = pd.DataFrame(np_sig0p4_SignalLikeEvents, columns=allTrainingFeatures)\n",
    "  df_sig0p6_SignalLikeEvents = pd.DataFrame(np_sig0p6_SignalLikeEvents, columns=allTrainingFeatures)\n",
    "  df_sig0p8_SignalLikeEvents = pd.DataFrame(np_sig0p8_SignalLikeEvents, columns=allTrainingFeatures)\n",
    "  df_sig1p0_SignalLikeEvents = pd.DataFrame(np_sig1p0_SignalLikeEvents, columns=allTrainingFeatures)\n",
    "#  df_sig2p0_SignalLikeEvents = pd.DataFrame(np_sig2p0_SignalLikeEvents, columns=allTrainingFeatures)    \n",
    "    \n",
    "  sig_SignalLikeEvents = []  \n",
    "  for mass in sigMasses:\n",
    "    if mass == \"0p2\": sig_SignalLikeEvents.append(df_sig0p2_SignalLikeEvents)\n",
    "    if mass == \"0p4\": sig_SignalLikeEvents.append(df_sig0p4_SignalLikeEvents)\n",
    "    if mass == \"0p6\": sig_SignalLikeEvents.append(df_sig0p6_SignalLikeEvents)\n",
    "    if mass == \"0p8\": sig_SignalLikeEvents.append(df_sig0p8_SignalLikeEvents)\n",
    "    if mass == \"1p0\": sig_SignalLikeEvents.append(df_sig1p0_SignalLikeEvents)\n",
    "#    if mass == \"2p0\": sig_SignalLikeEvents.append(df_sig2p0_SignalLikeEvents)\n",
    "    \n",
    "    \n",
    "  plotVariables(sig_SignalLikeEvents, sigMasses, df_bkg_SignalLikeEvents, allTrainingFeatures)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e9a7d-c6cc-4da5-a707-01dc7083d4e2",
   "metadata": {},
   "source": [
    "BDT results for the same sign MC events. Not used anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b4608-26d4-4c28-9e67-2f84e6c18f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "  stacut = -0.6\n",
    "  filter = (weightedMcInputs_SameSign['standaloneDEoverE']<stacut) & (weightedMcInputs_SameSign['cellEdgeDeta']>0.004) & (weightedMcInputs_SameSign['cellEdgeDphi']>0.016)\n",
    "  filteredData = weightedMcInputs_SameSign[filter]\n",
    "\n",
    "  print(len(filteredData))\n",
    "  filteredData.drop('cellEdgeDphi', inplace=True, axis=1)\n",
    "  filteredData.drop('cellEdgeDeta', inplace=True, axis=1)\n",
    "  missingOneHit_SameSign = filteredData.copy()\n",
    " # missingOneHit_SameSign = missingOneHit_SameSign[((missingOneHit_SameSign['HEDepth_0']==0) | (missingOneHit_SameSign['HEDepth_1']==0) | (missingOneHit_SameSign['HEDepth_2']==0) | (missingOneHit_SameSign['HEDepth_3']==0) | (missingOneHit_SameSign['HEDepth_4']==0) | (missingOneHit_SameSign['HEDepth_5']==0) | (missingOneHit_SameSign['HEDepth_6']==0))]\n",
    "\n",
    "  print(len(missingOneHit_SameSign))\n",
    "  missingOneHit_SameSign_x,missingOneHit_SameSign_y,missingOneHit_SameSign_massess = format_data(missingOneHit_SameSign,allTrainingFeatures)\n",
    "  missingOneHit_SameSign_weights = missingOneHit_SameSign_x[:,missingOneHit_SameSign_x.shape[1]-1]\n",
    "  missingOneHit_SameSign_x = np.delete(missingOneHit_SameSign_x,missingOneHit_SameSign_x.shape[1]-1,1)            \n",
    "\n",
    "  SameSign_pred = []\n",
    "  for i in range(0,nFolds):\n",
    "    SameSign_pred.append(bdtArray[i].predict(missingOneHit_SameSign_x))\n",
    "  \n",
    "  print(SameSign_pred)\n",
    "  np.histogram(SameSign_pred[1],bins=np.arange(0,1.01,0.04))\n",
    "#  axes = SameSign_pred[0].hist(column='BDTscore',log=True,\n",
    "#                               bins=np.arange(0,1.01,0.04),\n",
    "#                               label='Same Sign BDT 0')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433649f2-210f-4d36-8431-5f296f20aade",
   "metadata": {},
   "source": [
    "BDT results for the same sign data events. Used to predict the muPX background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8154920-1215-4c55-b36b-813be1761970",
   "metadata": {},
   "outputs": [],
   "source": [
    "  stacut = -0.6\n",
    "  filter = (dataInputs_SameSign['standaloneDEoverE']<stacut) & (dataInputs_SameSign['cellEdgeDeta']>0.004) & (dataInputs_SameSign['cellEdgeDphi']>0.016)\n",
    "  filteredData = dataInputs_SameSign[filter]\n",
    "    \n",
    "  filteredData.drop('cellEdgeDphi', inplace=True, axis=1)\n",
    "  filteredData.drop('cellEdgeDeta', inplace=True, axis=1)\n",
    "  missingOneHit_data_SameSign = filteredData.copy()\n",
    "\n",
    "  data_SameSign_x,data_SameSign_y,data_SameSign_massess = format_data(missingOneHit_data_SameSign,allTrainingFeatures)\n",
    "  data_SameSign_weights = data_SameSign_x[:,data_SameSign_x.shape[1]-1]\n",
    "  data_SameSign_x = np.delete(data_SameSign_x,data_SameSign_x.shape[1]-1,1)            \n",
    "\n",
    "  SameSign_pred = []\n",
    "  for i in range(0,nFolds):\n",
    "    SameSign_pred.append(bdtArray[i].predict(data_SameSign_x))\n",
    "  \n",
    "  print(SameSign_pred)\n",
    "  counts_sameSign, binedges = np.histogram(SameSign_pred[1],bins=np.arange(0,1.01,0.04))\n",
    "  print(binedges)\n",
    "  print(counts_sameSign)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84989374-71a3-4ba1-92bc-6b646cd26b28",
   "metadata": {},
   "source": [
    "BDT prediction of the ECAL Brem signal samples. Manipulates the dataframes in the same way the original 'fixB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a8d4f-3219-41a2-9314-8befbd7d2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "  sigPreds_ecalBrem = []\n",
    "  stacut = -0.6\n",
    "  filter = (allSignals_ecalBrem['standaloneDEoverE']<stacut) & (allSignals_ecalBrem['cellEdgeDeta']>0.004) & (allSignals_ecalBrem['cellEdgeDphi']>0.016)\n",
    "  filteredData = allSignals_ecalBrem[filter]\n",
    "  signal_ecalBrem_SystWeights = filteredData.copy()\n",
    "    \n",
    "  filteredData.drop('cellEdgeDphi', inplace=True, axis=1)\n",
    "  filteredData.drop('cellEdgeDeta', inplace=True, axis=1)\n",
    "  filteredData.drop(['PUupWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['PUdownWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['IDupWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['IDdownWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['ISOupWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['ISOdownWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['TrigUpWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['TrigDownWeight'],inplace=True, axis=1)    \n",
    "  filteredData.drop(['EnBinWeight'],inplace=True, axis=1) \n",
    "  filteredData.drop(['EventWeight_cscReweight'],inplace=True, axis=1)     \n",
    "  print(filteredData)\n",
    "\n",
    "  signal_ecalBrem_SystWeights.drop(['pt'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['eta'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['phi'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['staDR'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['staPhi'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['staE'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['staChi'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['cscDR'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_0'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_1'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_2'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_3'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_4'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_5'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['HEDepth_6'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['probeCharge'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['standaloneDEoverE'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['cscDRbyStation_0'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['cscDRbyStation_1'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['cscDRbyStation_2'],inplace=True, axis=1)\n",
    "  signal_ecalBrem_SystWeights.drop(['cscDRbyStation_3'],inplace=True, axis=1)        \n",
    "    \n",
    "  signal_ecalBrem_x,signal_ecalBrem_y,signal_ecalBrem_massess = format_data(filteredData,allTrainingFeatures)\n",
    "  signal_ecalBrem_weights = signal_ecalBrem_x[:,signal_ecalBrem_x.shape[1]-1]\n",
    "  signal_ecalBrem_x = np.delete(signal_ecalBrem_x,signal_ecalBrem_x.shape[1]-1,1)            \n",
    "\n",
    "  signal_ecalBrem_pred = []\n",
    "  for i in range(0,nFolds):\n",
    "    signal_ecalBrem_pred.append(bdtArray[i].predict(signal_ecalBrem_x))\n",
    "  \n",
    "  print(signal_ecalBrem_pred)\n",
    "\n",
    "  #Using predictions of testing dataset\n",
    "  sig0p2_ecalBrem_preds = []\n",
    "  sig0p4_ecalBrem_preds = []\n",
    "  sig0p6_ecalBrem_preds = []\n",
    "  sig0p8_ecalBrem_preds = []\n",
    "  sig1p0_ecalBrem_preds = []\n",
    "  for idx2, BDTscore in enumerate(signal_ecalBrem_pred[0]):\n",
    "            if signal_ecalBrem_massess[idx2] == \"0p2\": sig0p2_ecalBrem_preds.append(BDTscore)\n",
    "            elif signal_ecalBrem_massess[idx2] == \"0p4\": sig0p4_ecalBrem_preds.append(BDTscore)\n",
    "            elif signal_ecalBrem_massess[idx2] == \"0p6\": sig0p6_ecalBrem_preds.append(BDTscore)\n",
    "            elif signal_ecalBrem_massess[idx2] == \"0p8\": sig0p8_ecalBrem_preds.append(BDTscore)\n",
    "            elif signal_ecalBrem_massess[idx2] == \"1p0\": sig1p0_ecalBrem_preds.append(BDTscore)\n",
    "    \n",
    "  print(len(sig0p2_ecalBrem_preds))\n",
    "  print(len(sig0p4_ecalBrem_preds))\n",
    "  print(len(sig0p6_ecalBrem_preds))\n",
    "  print(len(sig0p8_ecalBrem_preds))\n",
    "  print(len(sig1p0_ecalBrem_preds))\n",
    "    \n",
    "  sigPreds_ecalBrem.append(sig0p4_ecalBrem_preds)\n",
    "  sigPreds_ecalBrem.append(sig0p8_ecalBrem_preds)\n",
    "  sigPreds_ecalBrem.append(sig0p6_ecalBrem_preds)\n",
    "  sigPreds_ecalBrem.append(sig0p2_ecalBrem_preds)\n",
    "  sigPreds_ecalBrem.append(sig1p0_ecalBrem_preds)    \n",
    " \n",
    "  #sigPreds_ecalBrem.append(sig0p4_ecalBrem_preds)\n",
    "  #sigPreds_ecalBrem.append(sig0p6_ecalBrem_preds)\n",
    "  #sigPreds_ecalBrem.append(sig0p2_ecalBrem_preds)\n",
    "  #sigPreds_ecalBrem.append(sig0p8_ecalBrem_preds)\n",
    "  #sigPreds_ecalBrem.append(sig1p0_ecalBrem_preds)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e00b8-9cf2-4cf9-8648-9db09edcdffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "  stacut = -0.6\n",
    "  filter = (weightedMCInputs_highHCAL['standaloneDEoverE']<100) \n",
    "  filteredMC = weightedMCInputs_highHCAL[filter]    \n",
    "  filteredMC.drop('cellEdgeDphi', inplace=True, axis=1)\n",
    "  filteredMC.drop('cellEdgeDeta', inplace=True, axis=1)\n",
    "  missingOneHit_highHCAL = filteredMC.copy()\n",
    "\n",
    "  filter = (dataInputs_highHCAL['standaloneDEoverE']<100)\n",
    "  filteredData = dataInputs_highHCAL[filter]\n",
    "  filteredData.drop('cellEdgeDphi', inplace=True, axis=1)\n",
    "  filteredData.drop('cellEdgeDeta', inplace=True, axis=1)\n",
    "  missingOneHitData_highHCAL = filteredData.copy()\n",
    "\n",
    "  missingOneHit_highHCAL_x,missingOneHit_highHCAL_y,missingOneHit_highHCAL_massess = format_data(missingOneHit_highHCAL,allTrainingFeatures)\n",
    "  missingOneHit_highHCAL_weights = missingOneHit_highHCAL_x[:,missingOneHit_highHCAL_x.shape[1]-1]\n",
    "  missingOneHit_highHCAL_x = np.delete(missingOneHit_highHCAL_x,missingOneHit_highHCAL_x.shape[1]-1,1)            \n",
    "\n",
    "  missingOneHitData_highHCAL_x,missingOneHitData_highHCAL_y,missingOneHitData_highHCAL_massess = format_data(missingOneHitData_highHCAL,allTrainingFeatures)\n",
    "  missingOneHitData_highHCAL_weights = missingOneHitData_highHCAL_x[:,missingOneHitData_highHCAL_x.shape[1]-1]\n",
    "  missingOneHitData_highHCAL_x = np.delete(missingOneHitData_highHCAL_x,missingOneHitData_highHCAL_x.shape[1]-1,1)  \n",
    "    \n",
    "  MC_highHCAL_pred = []\n",
    "  Data_highHCAL_pred = []\n",
    "  for i in range(0,nFolds):\n",
    "    MC_highHCAL_pred.append(bdtArray[i].predict(missingOneHit_highHCAL_x))\n",
    "    Data_highHCAL_pred.append(bdtArray[i].predict(missingOneHitData_highHCAL_x))\n",
    "    \n",
    "  pd_MC_highHCAL_pred = pd.DataFrame({'BDTscore':MC_highHCAL_pred[0],'eventWeights':missingOneHit_highHCAL_weights}) \n",
    "\n",
    "  MC_highHCAL_Weights = pd_MC_highHCAL_pred['eventWeights']\n",
    "  MC_highHCAL_Weights_unity = MC_highHCAL_Weights/(MC_highHCAL_Weights.sum())\n",
    "  \n",
    "  pd_Data_highHCAL_pred = pd.DataFrame({'BDTscore':Data_highHCAL_pred[0]})\n",
    "#  pd_Data_highHCAL_pred.hist(column='BDTscore',log=True,\n",
    "#                             ax=axes,histtype='step',hatch='/', edgecolor=\"Red\",                             \n",
    "#                             bins=np.arange(0,1.01,0.04),label='Data (BDT 0)')\n",
    "\n",
    "  bin_edges = np.arange(0,1.04,0.05)         \n",
    "  bin_centers = (bin_edges[:-1] + bin_edges[1:])/2.  \n",
    "  #axes.errorbars(bin_centers, )\n",
    "        \n",
    "  \n",
    "  np_Data_highHCAL_pred = pd_Data_highHCAL_pred.to_numpy()\n",
    "  counts_data, binedges = np.histogram(np_Data_highHCAL_pred[:,0], bins=bin_edges)\n",
    "  np_counts_data = np.asarray(counts_data)\n",
    "  #counts_bkg_sqrt = np.sqrt(np_counts_bkg)/(testBkgWeights.sum()*0.04)\n",
    "    \n",
    "  counts_data_sqrt = []\n",
    "  totalCounts = 0\n",
    "  for counts in counts_data:\n",
    "    error = np.sqrt(counts)\n",
    "    counts_data_sqrt.append(error)\n",
    "    totalCounts = totalCounts + counts\n",
    "    \n",
    "  print(len(counts_data))\n",
    "  print(len(counts_data_sqrt))\n",
    "  print(counts_data_sqrt)\n",
    "\n",
    "  np_MC_highHCAL_pred = pd_MC_highHCAL_pred.to_numpy()\n",
    "  counts_MC, binedges = np.histogram(np_MC_highHCAL_pred[:,0], bins=bin_edges,weights=MC_highHCAL_Weights_unity*totalCounts)\n",
    "#  counts_MC, binedges = np.histogram(np_MC_highHCAL_pred[:,0], bins=bin_edges,weights=MC_highHCAL_Weights)\n",
    "  np_counts_MC = np.asarray(counts_MC)\n",
    "    \n",
    "  counts_MC_sqrt = []\n",
    "  for bin_index in range(len(binedges) - 1):  \n",
    "    bin_left = binedges[bin_index]\n",
    "    bin_right = binedges[bin_index + 1]\n",
    "    in_bin = np.logical_and(bin_left < np_MC_highHCAL_pred[:,0], np_MC_highHCAL_pred[:,0] <= bin_right)\n",
    "\n",
    "    # filter the weights to only those inside the bin\n",
    "    weights_in_bin = MC_highHCAL_Weights_unity[in_bin]*totalCounts\n",
    "#    weights_in_bin = MC_highHCAL_Weights[in_bin]\n",
    "\n",
    "    # compute the error however you want\n",
    "    error = np.sqrt(np.sum(weights_in_bin ** 2))\n",
    "    counts_MC_sqrt.append(error)\n",
    "\n",
    "\n",
    "\n",
    "  plt.errorbar(bin_centers, counts_MC, yerr=counts_MC_sqrt, fmt='s', ecolor='Blue',\n",
    "               mfc='Blue',mec='Blue',label='MC (BDT 0)')\n",
    "\n",
    "#  axes = pd_MC_highHCAL_pred.hist(column='BDTscore',log=True,\n",
    "#                                    bins=np.arange(0,1.04,0.05),\n",
    "#                                    weights=MC_highHCAL_Weights_unity*totalCounts,\n",
    "                                    #weights=pd_MC_highHCAL_pred['eventWeights']*59.8*398*1000*1.18*10**(-6),\n",
    "#                                    label='MC (BDT 0)')\n",
    "\n",
    "\n",
    "  plt.errorbar(bin_centers, counts_data, yerr=counts_data_sqrt, fmt='o', ecolor='Red',\n",
    "               mfc='Red',mec='Red',label='Data (BDT 0)')\n",
    "\n",
    "  plt.xlabel(\"BDT score\", loc='right')\n",
    "  plt.ylabel(\"# of events\", loc='top')   \n",
    "  plt.legend(loc='upper center')#,ncol=2)\n",
    "  plt.grid(False)    \n",
    "  plt.yscale(\"log\")    \n",
    "  plt.title(\" \")\n",
    "  plt.savefig(\"BDT_highHCALE.pdf\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a999a4-522d-405f-a7fc-b71f7d5738f2",
   "metadata": {},
   "source": [
    "Plot the ROC curves for each of the test BDTs. Multiplies the false and true positive rates by the standalone muon selection efficiency so that the curve is made for the whole dataset to allow for proper comparisons to the cut-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec3aa7-d464-41ba-8aa4-085297c615c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, curve in enumerate(curves):\n",
    "   print(idx)\n",
    "   print(sigMasses[idx])\n",
    "   fig = plt.figure(figsize = (8,8))\n",
    "   ax = fig.add_subplot(1,1,1)\n",
    "   ax.set_xlabel('False Positive Rate', fontsize = 15)\n",
    "   ax.set_ylabel('True Positive Rate', fontsize = 15)\n",
    "   ax.set_title('ROC Curve', fontsize = 20)\n",
    "#   ax.set_title('ROC Curve, '+sigMasses[idx], fontsize = 20)\n",
    "   fpr = curve[0]\n",
    "   tpr = curve[1]\n",
    "   fpralt = curve[2]*curve[4]\n",
    "   tpralt = curve[3]*curve[5]\n",
    "   fpralt_noC = curve[6]*curve[8]\n",
    "   tpralt_noC = curve[7]*curve[9]\n",
    "   xvar = fpr*curve[4]\n",
    "   yvar = tpr*curve[5]\n",
    "   #ax.plot(xvar, yvar, '-o',linewidth=3,markersize=0) \n",
    "   ax.plot(fpralt,tpralt, '-go',linewidth=3,markersize=0)\n",
    "#   ax.plot(fpralt_noC,tpralt_noC, '-ro',linewidth=3,markersize=0)\n",
    "   print(\"truePos\")\n",
    "   print(truePos)\n",
    "   ax.plot(falsePos,truePos[idx],'ko')\n",
    "   ax.grid()\n",
    "   ax.set_xscale('log')\n",
    "   ax.set_yscale('log')    \n",
    "#   ax.legend(['BDT','BDT (no track charge)','Cut Based Estimate'])\n",
    "#   ax.legend(['Scale POS Weight '+str(pos_weight),'Cut Based Estimate'])\n",
    "#   ax.legend(['Logistic Regression','Scale POS Weight '+str(pos_weight),'No HE Information','Cut Based Estimate'])\n",
    "   plt.xlim(0.000003,0.01)\n",
    "   fig.savefig(\"partialRegionBDTRocCurve.pdf\")\n",
    "\n",
    "   ax = importance[idx]\n",
    "   fig = ax.figure\n",
    "   fig.tight_layout()\n",
    "   fig.savefig(\"partialBDTImportance.pdf\")\n",
    "\n",
    "   #ax1 = importance[idx+1]\n",
    "   #fig1 = ax1.figure\n",
    "   #fig1.tight_layout()\n",
    "   #fig1.savefig(\"partialBDTImportance_noTrackCharge.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de12a6-69ae-4657-9598-4f3f4d7acd67",
   "metadata": {},
   "source": [
    "This part makes the dictionaries for my combine input maker. I had trouble making root files with jupyter lab (the python versions are incompatible, so I'd need to switch to a CERN made version of jupyter) so this writes the BDT prediction for each DY and signal point along with the corresponding weights directly to a pickle file, which I load in and then iterate over to fill a histogram in my combine input producer. Maybe more efficient to just used the saved BDTs to re-make the predictions in the other script, but as the file sizes aren't too big and I already have the predictions I did it this way. I also considered just saving the bin edges and counts, but getting the uncertainties correct is a bit of a pain that way.\n",
    "\n",
    "The file is structured as a list of dictionaries. The first six dictionaries each correspond to a signal mass, and have the mass, standalone muon selection efficiency, DY predictions, signal predictions, and systematic variations for the signal events at that mass.\n",
    "The final dictionary contains the event weights for the DY samples with each systematic variation, mainly so that the same information isn't repeated in every signal dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ad06d-0585-4961-911e-2d0bae9e17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDicts=[]\n",
    "DY_BDToutput_systWeights = BDToutput_systWeights[BDToutput_systWeights['label']<0.5]\n",
    "DY_BDToutput_systWeights.drop('label', inplace=True, axis=1)\n",
    "DY_BDToutput_systWeights.drop('massPoint', inplace=True, axis=1)\n",
    "print(len(newGenPreds[0]))\n",
    "print(len(sigPreds))\n",
    "print(len(curves))\n",
    "print(len(DY_BDToutput_systWeights))\n",
    "for idx, mass in enumerate(sigMasses):\n",
    "    print(idx)\n",
    "    print(mass)\n",
    "    sigSplitSystWeights = BDToutput_systWeights[BDToutput_systWeights['massPoint']==mass]\n",
    "    sigSplitSystWeights.drop(['label'], inplace=True, axis=1)\n",
    "    sigSplitSystWeights_ecalBrem = signal_ecalBrem_SystWeights[signal_ecalBrem_SystWeights['massPoint']==mass]\n",
    "    sigSplitSystWeights_ecalBrem.drop(['label'], inplace=True, axis=1)\n",
    "    massDict = {}\n",
    "    massDict['name']=mass\n",
    "#    massDict['dyEff']=curves[0][8]\n",
    "#    massDict['sigEff']=curves[0][9]\n",
    "#    massDict['mcPreds']=newGenPreds[1]\n",
    "#    massDict['sigPreds']=sigPreds[idx+6]\n",
    "    massDict['dyEff']=curves[0][4]\n",
    "    massDict['sigEff']=curves[0][5]    \n",
    "    massDict['mcPreds']=newGenPreds[0]\n",
    "    massDict['sigPreds']=sigPreds[idx]\n",
    "    massDict['muPXPreds']=SameSign_pred[0]\n",
    "    print(len(sigPreds_ecalBrem[idx]))\n",
    "    massDict['sigPreds_ecalBrem']=sigPreds_ecalBrem[idx]    \n",
    "    print(\"len(sigPreds[idx])\")\n",
    "    print(len(sigPreds[idx]))\n",
    "    print(len(sigSplitSystWeights))\n",
    "    print(len(sigPreds_ecalBrem[idx]))\n",
    "    print(len(sigSplitSystWeights_ecalBrem))\n",
    "    for name, Systweights in sigSplitSystWeights.iteritems():\n",
    "        massDict[name]=Systweights.values\n",
    "    for name, Systweights in sigSplitSystWeights_ecalBrem.iteritems():\n",
    "        massDict[name+'_ecalBrem']=Systweights.values        \n",
    "    outputDicts.append(massDict)        \n",
    "dyWeightsDict={}\n",
    "for name,Systweights in DY_BDToutput_systWeights.iteritems():\n",
    "    dyWeightsDict[name]=Systweights.values\n",
    "print(len(dyWeightsDict))\n",
    "outputDicts.append(dyWeightsDict)\n",
    "pickle.dump(outputDicts, open('bdtPredictions'+sigMasses[4]+'.pkl', 'wb'),protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee3123-e4bd-4474-9a81-1e11dac8b069",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = ['disStaDEoverE', 'disStadEta', 'disStaChi2', 'refitStadEta', 'refitStaChi2', 'refitStaDEoverE', 'disStadPhi', 'refitStadPhi', 'refitStaE']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
